{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "dgBTU57OWYI0"
   },
   "source": [
    "# Install Library\n",
    "\n",
    "[RDKit ](https://github.com/rdkit/rdkit)\n",
    "\n",
    "[DGL](https://github.com/dmlc/dgl/)\n",
    "\n",
    "[DGL-LifeSci](https://github.com/awslabs/dgl-lifesci)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "EOF1QxeqhajG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rdkit-pypi\n",
    "!pip install dgllife\n",
    "!pip install --pre dgl-cu113 dglgo -f https://data.dgl.ai/wheels-test/repo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "xtojkovzWYI2"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl \n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "import torchvision\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from dgllife.model import MLPPredictor\n",
    "from tensorflow.keras.callbacks import  History\n",
    "from dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer, AttentiveFPAtomFeaturizer\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from utils.general import DATASET, get_dataset, separate_active_and_inactive_data, get_embedding_vector_class, count_lablel,data_generator, up_and_down_Samplenig\n",
    "#from utils.gcn_pre_trained import get_tox21_model\n",
    "#from model.heterogeneous_siamese_tox21 import siamese_model_attentiveFp_tox21, siamese_model_Canonical_tox21\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general import DATASET, get_dataset, separate_active_and_inactive_data, get_embedding_vector_class, count_lablel,data_generator, up_and_down_Samplenig\n",
    "from gcn_pre_trained import get_tox21_model\n",
    "from heterogeneous_siamese_tox21 import siamese_model_attentiveFp_tox21, siamese_model_Canonical_tox21\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "1RVgRpTmQ5rp",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cache_path='./tox21_dglgraph.bin'\n",
    "\n",
    "df = get_dataset(\"tox21\")\n",
    "id = df['mol_id']\n",
    "df = df.drop(columns=['mol_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "id": "m_nAHT_WhajK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n"
     ]
    }
   ],
   "source": [
    "tox21_tasks = df.columns.values[:12].tolist()\n",
    "print(tox21_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "GEdp1FUphajL",
    "outputId": "352d95e6-5d5c-4c98-8d51-d7f330c51e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR-AR one: 309  zero: 6956  NAN: 566\n",
      "NR-AR-LBD one: 237  zero: 6521  NAN: 1073\n",
      "NR-AhR one: 768  zero: 5781  NAN: 1282\n",
      "NR-Aromatase one: 300  zero: 5521  NAN: 2010\n",
      "NR-ER one: 793  zero: 5400  NAN: 1638\n",
      "NR-ER-LBD one: 350  zero: 6605  NAN: 876\n",
      "NR-PPAR-gamma one: 186  zero: 6264  NAN: 1381\n",
      "SR-ARE one: 942  zero: 4890  NAN: 1999\n",
      "SR-ATAD5 one: 264  zero: 6808  NAN: 759\n",
      "SR-HSE one: 372  zero: 6095  NAN: 1364\n",
      "SR-MMP one: 918  zero: 4892  NAN: 2021\n",
      "SR-p53 one: 423  zero: 6351  NAN: 1057\n"
     ]
    }
   ],
   "source": [
    "one = []\n",
    "zero = []\n",
    "nan = []\n",
    " \n",
    "for task in tox21_tasks:\n",
    "    a = list(df[task].value_counts(dropna=False).to_dict().values())\n",
    "    zero.append(a[0])\n",
    "    nan.append(a[1])\n",
    "    one.append(a[2])\n",
    "    print(task ,\"one:\" ,a[2] ,\" zero:\", a[0], \" NAN:\",a[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5862, 72084)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(one), sum(zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "hidden": true,
    "id": "QjNMCFSqhajM",
    "outputId": "112c0f92-1e31-467c-e3b5-a9285f9c9f14"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the matplotlib library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Declaring the figure or the plot (y, x) or (width, height)\n",
    "plt.figure(figsize=[20, 15])\n",
    "\n",
    "X = np.arange(1,len(tox21_tasks)+1)\n",
    "plt.bar(X + 0.2, one, color = 'g', width = 0.25)\n",
    "plt.bar(X + 0.4, zero, color = 'b', width = 0.25)\n",
    "plt.bar(X + 0.6, nan, color = 'r', width = 0.25)\n",
    "\n",
    "# Creating the legend of the bars in the plot\n",
    "plt.legend(['Active' , 'Inactive' ,'NAN'])\n",
    "# Overiding the x axis with the country names\n",
    "plt.xticks([i + 0.25 for i in range(1,13)], X)\n",
    "# Giving the tilte for the plot\n",
    "plt.title(\"Tox21 dataset diagram\")\n",
    "# Namimg the x and y axis\n",
    "plt.xlabel('Tox21_tasks')\n",
    "plt.ylabel('Cases')\n",
    "# Saving the plot as a 'png'\n",
    "plt.savefig('4BarPlot.png')\n",
    "# Displaying the bar plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "6grIE_JeqkUZ"
   },
   "source": [
    "# Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "id": "IzllOg474i99"
   },
   "outputs": [],
   "source": [
    "from dgllife.model import MLPPredictor\n",
    "\n",
    "def create_dataset_with_gcn(dataset, class_embed_vector, GCN, tasks, numberTask):\n",
    "\n",
    "    created_data = []\n",
    "    data = np.arange(len(tasks))\n",
    "    onehot_encoded = to_categorical(data)\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        smiles, g, label, mask = data\n",
    "        print(smiles)\n",
    "        g = g.to(device)\n",
    "        g = dgl.add_self_loop(g)\n",
    "        graph_feats = g.ndata.pop('h')\n",
    "        embbed = GCN(g, graph_feats)\n",
    "        print(embbed)\n",
    "        embbed = embbed.to('cpu')\n",
    "        embbed = embbed.detach().numpy()\n",
    "        print(embbed)\n",
    "        raise ValueException('ok')\n",
    "        a = ( embbed, onehot_encoded[numberTask], class_embed_vector[numberTask], label, numberTask, tasks[numberTask])\n",
    "        created_data.append(a)\n",
    "    print('Data created!!')\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Calculation of embedded vectors for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR-AR=> positive: 309 - negative: 6956\n",
      "NR-AR-LBD=> positive: 237 - negative: 6521\n",
      "NR-AhR=> positive: 768 - negative: 5781\n",
      "NR-Aromatase=> positive: 300 - negative: 5521\n",
      "NR-ER=> positive: 793 - negative: 5400\n",
      "NR-ER-LBD=> positive: 350 - negative: 6605\n",
      "NR-PPAR-gamma=> positive: 186 - negative: 6264\n",
      "SR-ARE=> positive: 942 - negative: 4890\n",
      "SR-ATAD5=> positive: 264 - negative: 6808\n",
      "SR-HSE=> positive: 372 - negative: 6095\n",
      "SR-MMP=> positive: 918 - negative: 4892\n",
      "SR-p53=> positive: 423 - negative: 6351\n"
     ]
    }
   ],
   "source": [
    "df_positive, df_negative = separate_active_and_inactive_data(df, tox21_tasks)\n",
    "\n",
    "for i,d in enumerate(zip(df_positive,df_negative)):\n",
    "    print(f'{tox21_tasks[i]}=> positive: {len(d[0])} - negative: {len(d[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6956\n",
      "Processing molecule 2000/6956\n",
      "Processing molecule 3000/6956\n",
      "Processing molecule 4000/6956\n",
      "Processing molecule 5000/6956\n",
      "Processing molecule 6000/6956\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6521\n",
      "Processing molecule 2000/6521\n",
      "Processing molecule 3000/6521\n",
      "Processing molecule 4000/6521\n",
      "Processing molecule 5000/6521\n",
      "Processing molecule 6000/6521\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5781\n",
      "Processing molecule 2000/5781\n",
      "Processing molecule 3000/5781\n",
      "Processing molecule 4000/5781\n",
      "Processing molecule 5000/5781\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5521\n",
      "Processing molecule 2000/5521\n",
      "Processing molecule 3000/5521\n",
      "Processing molecule 4000/5521\n",
      "Processing molecule 5000/5521\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5400\n",
      "Processing molecule 2000/5400\n",
      "Processing molecule 3000/5400\n",
      "Processing molecule 4000/5400\n",
      "Processing molecule 5000/5400\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6605\n",
      "Processing molecule 2000/6605\n",
      "Processing molecule 3000/6605\n",
      "Processing molecule 4000/6605\n",
      "Processing molecule 5000/6605\n",
      "Processing molecule 6000/6605\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6264\n",
      "Processing molecule 2000/6264\n",
      "Processing molecule 3000/6264\n",
      "Processing molecule 4000/6264\n",
      "Processing molecule 5000/6264\n",
      "Processing molecule 6000/6264\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/4890\n",
      "Processing molecule 2000/4890\n",
      "Processing molecule 3000/4890\n",
      "Processing molecule 4000/4890\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6808\n",
      "Processing molecule 2000/6808\n",
      "Processing molecule 3000/6808\n",
      "Processing molecule 4000/6808\n",
      "Processing molecule 5000/6808\n",
      "Processing molecule 6000/6808\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6095\n",
      "Processing molecule 2000/6095\n",
      "Processing molecule 3000/6095\n",
      "Processing molecule 4000/6095\n",
      "Processing molecule 5000/6095\n",
      "Processing molecule 6000/6095\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/4892\n",
      "Processing molecule 2000/4892\n",
      "Processing molecule 3000/4892\n",
      "Processing molecule 4000/4892\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6351\n",
      "Processing molecule 2000/6351\n",
      "Processing molecule 3000/6351\n",
      "Processing molecule 4000/6351\n",
      "Processing molecule 5000/6351\n",
      "Processing molecule 6000/6351\n"
     ]
    }
   ],
   "source": [
    "dataset_positive = [DATASET(d,smiles_to_bigraph, AttentiveFPAtomFeaturizer(), cache_file_path = cache_path) for d in df_positive]\n",
    "dataset_negative = [DATASET(d,smiles_to_bigraph, AttentiveFPAtomFeaturizer(), cache_file_path = cache_path) for d in df_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class vector created!!\n"
     ]
    }
   ],
   "source": [
    "embed_class_tox21 = get_embedding_vector_class(dataset_positive, dataset_negative, radius=2, size = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "zIKQi__XAcia"
   },
   "source": [
    "# Classification with BioAct-Het and AttentiveFp GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "lDS5UguKr_x_",
    "outputId": "da58be7e-197e-4838-f5f1-a0b2d6b87cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GCN_attentivefp_Tox21_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gcn_attentivefp_tox21.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN_attentivefp_Tox21_pre_trained.pth: 100%|██████████████████████| 1.95M/1.95M [00:00<00:00, 3.51MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'GCN_attentivefp_Tox21'\n",
    "gcn_model = get_tox21_model(model_name)\n",
    "gcn_model.eval()\n",
    "gcn_model = gcn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/7265\n",
      "Processing molecule 2000/7265\n",
      "Processing molecule 3000/7265\n",
      "Processing molecule 4000/7265\n",
      "Processing molecule 5000/7265\n",
      "Processing molecule 6000/7265\n",
      "Processing molecule 7000/7265\n",
      "CCOc1ccc2nc(S(N)(=O)=O)sc2c1\n",
      "tensor([[ 0.0722,  0.1108,  0.0606, -0.0351, -0.1969, -0.0841,  0.0209, -0.2597,\n",
      "          0.0051, -0.0519,  0.2201,  0.3521, -0.1576,  0.0616,  0.2195, -0.1216,\n",
      "         -0.0166,  0.0235,  0.1007, -0.2479, -0.0882, -0.3130, -0.0249,  0.0567,\n",
      "          0.0183,  0.0276,  0.0025, -0.0129, -0.1225, -0.0047,  0.2012,  0.1449,\n",
      "         -0.3829,  0.0254, -0.1366,  0.0022,  0.0644,  0.0196,  0.2234, -0.1682,\n",
      "         -0.1664, -0.1468,  0.0289, -0.0953,  0.0899, -0.1570,  0.1611, -0.1050,\n",
      "         -0.0537,  0.0582,  0.1174,  0.1112,  0.1032,  0.1274,  0.1687,  0.0364,\n",
      "          0.1750, -0.0183,  0.0853, -0.0737,  0.0723, -0.1192,  0.0799,  0.0343,\n",
      "         -0.1244, -0.0125,  0.1075,  0.0827,  0.0333,  0.1836,  0.0491, -0.1296,\n",
      "         -0.0809,  0.0597,  0.1296,  0.2187, -0.0926, -0.1374, -0.0769,  0.0725,\n",
      "         -0.0627,  0.0063, -0.0231,  0.0404,  0.0437,  0.1851,  0.1329, -0.0408,\n",
      "          0.1312, -0.0248, -0.1921,  0.0753,  0.1688, -0.0726,  0.1071,  0.0242,\n",
      "         -0.2535,  0.1015,  0.1227, -0.0205, -0.0270, -0.0630, -0.0302,  0.1017,\n",
      "         -0.0447, -0.0018,  0.1957,  0.0750, -0.0185,  0.0301, -0.1543, -0.1171,\n",
      "          0.0327,  0.1538,  0.0183, -0.1397,  0.2062,  0.1122, -0.0178, -0.0682,\n",
      "         -0.0988,  0.0498,  0.1822, -0.1049, -0.0024,  0.0444,  0.1909,  0.0600,\n",
      "          0.1957, -0.0429, -0.2145,  0.1068,  0.1405,  0.1676, -0.0346,  0.1713,\n",
      "          0.0424,  0.0545,  0.2218,  0.0131,  0.0357, -0.1889,  0.0422,  0.0125,\n",
      "          0.0302, -0.1434,  0.0084, -0.0214, -0.1688, -0.0110,  0.0784, -0.0577,\n",
      "          0.0783,  0.1042, -0.0427,  0.2225,  0.2281, -0.1699, -0.1630,  0.0724,\n",
      "         -0.3389, -0.2206, -0.1597, -0.0576, -0.0228, -0.1753,  0.1210, -0.1956,\n",
      "         -0.0557,  0.2716,  0.0438, -0.0765, -0.1735,  0.0694, -0.0336, -0.0004,\n",
      "          0.0767,  0.0181,  0.0350, -0.1611, -0.0700, -0.2159,  0.2796,  0.1029,\n",
      "          0.1032, -0.1259, -0.2883,  0.0145,  0.2819, -0.1543,  0.2738, -0.1722,\n",
      "          0.0197, -0.0519, -0.1168,  0.1169, -0.1873, -0.1266,  0.1548, -0.1621,\n",
      "         -0.1198, -0.1920,  0.0373,  0.0825,  0.1104, -0.0091,  0.0279,  0.2790,\n",
      "         -0.0145, -0.2990,  0.0598, -0.1517, -0.0019,  0.0031,  0.1228,  0.0455,\n",
      "         -0.2068, -0.0334, -0.0994,  0.1200,  0.0309,  0.1276, -0.1803, -0.0491,\n",
      "          0.1599, -0.0892,  0.2735,  0.0024,  0.0362,  0.0237,  0.1032, -0.0264,\n",
      "          0.0796,  0.0365,  0.0095,  0.2152, -0.0295,  0.1527,  0.0301, -0.1882,\n",
      "          0.1961,  0.0319, -0.2008,  0.0511, -0.1534,  0.3303, -0.1523,  0.0522,\n",
      "          0.0787, -0.0340,  0.1769,  0.1221,  0.2339, -0.1740,  0.0099, -0.1295,\n",
      "         -0.0423, -0.0562, -0.0745,  0.1359, -0.0795,  0.0911, -0.0497,  0.0869,\n",
      "         -0.0900,  0.0139,  0.0429, -0.1764, -0.2928, -0.0652, -0.0011, -0.0173,\n",
      "         -0.3609,  0.1348, -0.0261,  0.1176,  0.0033,  0.2604, -0.0111, -0.0598,\n",
      "          0.0268, -0.0194,  0.2396,  0.0714, -0.0223,  0.0240, -0.0646, -0.0300,\n",
      "          0.0666,  0.2645, -0.0802, -0.0137, -0.0193,  0.0835,  0.0263,  0.1686,\n",
      "         -0.1326, -0.0532,  0.0226, -0.1344, -0.0931, -0.0239,  0.0832, -0.0237,\n",
      "          0.0428, -0.0454,  0.0772, -0.0863,  0.1728, -0.1959,  0.1092,  0.1083,\n",
      "         -0.0140,  0.0625,  0.0129,  0.0159, -0.0296, -0.1771, -0.1776, -0.1283,\n",
      "          0.1110, -0.0862,  0.1912,  0.2357,  0.0358,  0.1108,  0.2121, -0.1326,\n",
      "          0.1988,  0.0800, -0.0719, -0.1751, -0.0148,  0.0216,  0.0535,  0.0635,\n",
      "          0.0498,  0.0223,  0.0522,  0.0790,  0.0087, -0.0403, -0.0519, -0.1178,\n",
      "          0.2894,  0.0250, -0.0339, -0.0985, -0.1455, -0.1303,  0.0621,  0.1289,\n",
      "          0.0998, -0.0248, -0.1842, -0.0922,  0.1281, -0.0698, -0.0006, -0.1553,\n",
      "         -0.0168,  0.0930,  0.1826,  0.1691,  0.0137, -0.0436,  0.1471, -0.0231,\n",
      "         -0.2778,  0.0885, -0.0189, -0.1430, -0.1463, -0.0150, -0.0714,  0.0968,\n",
      "         -0.0145,  0.0203,  0.1297, -0.1611, -0.1308,  0.0317,  0.0488,  0.1081,\n",
      "         -0.1893,  0.0524,  0.0324, -0.0590,  0.0944,  0.0869, -0.0241, -0.2567,\n",
      "         -0.2150,  0.0435, -0.0467, -0.1251, -0.2754, -0.1788, -0.0504,  0.0905,\n",
      "         -0.0355, -0.0043,  0.0696,  0.2006,  0.3050, -0.1055, -0.0667,  0.0374,\n",
      "          0.1931,  0.0384,  0.2196, -0.1765,  0.2049, -0.1078,  0.2309,  0.1303,\n",
      "         -0.0454, -0.1204,  0.0712,  0.1218, -0.2123, -0.1129, -0.1397,  0.0354,\n",
      "         -0.0685, -0.0497,  0.0230, -0.1252,  0.1841, -0.0084,  0.2719,  0.1074,\n",
      "          0.0680, -0.1175,  0.2211,  0.1583, -0.0743, -0.0916, -0.1200, -0.0123,\n",
      "          0.1517, -0.0742, -0.1626, -0.0393, -0.1171,  0.0915,  0.1628, -0.0704,\n",
      "          0.1767, -0.0467, -0.0874, -0.0688, -0.0319, -0.0732,  0.0174,  0.2282,\n",
      "         -0.0336, -0.4059,  0.1935, -0.1527, -0.1255,  0.0038, -0.0420,  0.1432,\n",
      "          0.0199, -0.0869, -0.0921, -0.1533,  0.0440,  0.1665,  0.1361, -0.1381,\n",
      "         -0.2266,  0.0450,  0.1646, -0.0733,  0.0577, -0.2735,  0.0096,  0.0403,\n",
      "         -0.0499, -0.0150, -0.0784, -0.0608,  0.1566,  0.0779,  0.0635, -0.0778,\n",
      "         -0.0870, -0.0812, -0.0470,  0.0468,  0.0905,  0.1379,  0.0202,  0.0247,\n",
      "          0.1904, -0.1634, -0.0463,  0.1342, -0.0610,  0.0136,  0.0099,  0.0968,\n",
      "         -0.1285,  0.0458,  0.2396,  0.0634, -0.0421,  0.1843, -0.0031,  0.0293]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0.07217862  0.11082868  0.06059523 -0.03510134 -0.19686124 -0.08405089\n",
      "   0.02094857 -0.25973448  0.00507635 -0.05191197  0.22005731  0.3521198\n",
      "  -0.15756343  0.06159266  0.21954292 -0.1216184  -0.01656972  0.02350586\n",
      "   0.10069604 -0.24790077 -0.08822331 -0.3130446  -0.024893    0.05670877\n",
      "   0.01825423  0.0276296   0.00254369 -0.01291449 -0.12254141 -0.00472175\n",
      "   0.20123431  0.14488894 -0.3828732   0.02539993 -0.13659546  0.00218123\n",
      "   0.06435559  0.01958191  0.2233956  -0.1681684  -0.16635782 -0.14681862\n",
      "   0.02887799 -0.0953358   0.08993537 -0.15703833  0.1611286  -0.10498727\n",
      "  -0.05370038  0.05816339  0.11737294  0.11122981  0.10315503  0.12741935\n",
      "   0.16872783  0.03636547  0.1750471  -0.01831893  0.08531813 -0.07372813\n",
      "   0.0723305  -0.11916625  0.07992505  0.03426241 -0.12438285 -0.01247154\n",
      "   0.10750556  0.08265413  0.03325779  0.18361959  0.0491149  -0.12958837\n",
      "  -0.08092707  0.0597213   0.12959623  0.21874414 -0.09261233 -0.13740732\n",
      "  -0.07693607  0.07246126 -0.06273434  0.00634792 -0.0230572   0.04036224\n",
      "   0.04366476  0.1850978   0.13292031 -0.04078404  0.13124174 -0.02481495\n",
      "  -0.19210385  0.0753253   0.16876447 -0.07263616  0.10706048  0.02421551\n",
      "  -0.25348702  0.10146508  0.12274262 -0.02047426 -0.02696251 -0.06297538\n",
      "  -0.03015316  0.10173782 -0.04470202 -0.00176474  0.19567543  0.07502659\n",
      "  -0.01850344  0.0301135  -0.15433921 -0.1171222   0.03271542  0.1538061\n",
      "   0.01825877 -0.1396871   0.20620504  0.11224508 -0.01784858 -0.06822716\n",
      "  -0.09880597  0.04977687  0.18215735 -0.10492176 -0.00241971  0.04438628\n",
      "   0.19090222  0.06002229  0.1956618  -0.04285087 -0.21445337  0.10681757\n",
      "   0.14052029  0.16764757 -0.03460468  0.17127508  0.04243585  0.05446911\n",
      "   0.22180524  0.01305861  0.03565339 -0.18886478  0.04219409  0.01254915\n",
      "   0.03015238 -0.14341064  0.00842834 -0.02137295 -0.16884999 -0.01102936\n",
      "   0.07842925 -0.0576503   0.07829995  0.10415977 -0.0426781   0.22250359\n",
      "   0.22813657 -0.16990723 -0.16298217  0.07240856 -0.33885252 -0.22062817\n",
      "  -0.15972991 -0.05756538 -0.02275277 -0.17530447  0.12103048 -0.19558974\n",
      "  -0.05570808  0.27164742  0.04378832 -0.07654706 -0.1735233   0.06936198\n",
      "  -0.03360169 -0.00044286  0.07666306  0.01814064  0.03496987 -0.16112745\n",
      "  -0.07000685 -0.21592261  0.27960086  0.1028522   0.10321845 -0.12591337\n",
      "  -0.2882778   0.01454441  0.28190193 -0.15429135  0.2738123  -0.17218097\n",
      "   0.0197135  -0.05189253 -0.11681065  0.11688295 -0.18728618 -0.12658711\n",
      "   0.15479517 -0.16206436 -0.11980057 -0.19201003  0.03731856  0.08246337\n",
      "   0.11044984 -0.00907226  0.02789614  0.27899256 -0.01451561 -0.2989786\n",
      "   0.05977057 -0.15172589 -0.00188965  0.00309584  0.12283051  0.04551085\n",
      "  -0.20683439 -0.03343915 -0.09944589  0.12001717  0.03087446  0.12764046\n",
      "  -0.18031028 -0.04908853  0.1598578  -0.08920243  0.27349672  0.00241592\n",
      "   0.03622869  0.02368747  0.10315746 -0.02637699  0.07957353  0.03652589\n",
      "   0.00945152  0.21520245 -0.02948852  0.15270104  0.03014001 -0.1882449\n",
      "   0.19611527  0.03185465 -0.20084324  0.05107107 -0.15338716  0.33027917\n",
      "  -0.1523033   0.05216518  0.0787429  -0.03399483  0.1768973   0.12210344\n",
      "   0.23385496 -0.174047    0.0099164  -0.12950929 -0.04232874 -0.05622498\n",
      "  -0.07445921  0.13591847 -0.07945307  0.09108579 -0.04974984  0.08686073\n",
      "  -0.08997791  0.01388979  0.04288691 -0.17643127 -0.29280177 -0.06517082\n",
      "  -0.00107003 -0.01734836 -0.36088556  0.13475019 -0.02607498  0.11759664\n",
      "   0.00330429  0.26042864 -0.01112055 -0.0598024   0.02679158 -0.01939205\n",
      "   0.23957391  0.07144111 -0.02226537  0.0240401  -0.06463555 -0.02995992\n",
      "   0.06661707  0.26449442 -0.08024864 -0.01371201 -0.01925411  0.08346571\n",
      "   0.02626805  0.1685915  -0.13260224 -0.05324348  0.02258307 -0.13436514\n",
      "  -0.09313167 -0.02388325  0.08324388 -0.02367037  0.04282193 -0.04538389\n",
      "   0.07720215 -0.08632874  0.17283335 -0.19585818  0.10919446  0.1082993\n",
      "  -0.0139756   0.06249458  0.0128848   0.01588998 -0.02962856 -0.17708208\n",
      "  -0.17762323 -0.12828013  0.11103995 -0.08618668  0.19123022  0.23572019\n",
      "   0.03578977  0.11075176  0.21210024 -0.13263363  0.19880478  0.07997096\n",
      "  -0.07192539 -0.17506194 -0.01480049  0.02160519  0.0534573   0.06352991\n",
      "   0.04979013  0.02228734  0.05220284  0.07895458  0.00867074 -0.04033174\n",
      "  -0.05188242 -0.11780748  0.28944778  0.02496741 -0.03394696 -0.0985487\n",
      "  -0.14552435 -0.13030398  0.06205007  0.12891231  0.09983116 -0.02476875\n",
      "  -0.18421128 -0.09218454  0.1280694  -0.0697795  -0.00058663 -0.15534604\n",
      "  -0.01684399  0.09304543  0.18255416  0.16911215  0.01369461 -0.04359395\n",
      "   0.1471387  -0.02305209 -0.27777624  0.0885351  -0.01893793 -0.14295104\n",
      "  -0.14625765 -0.01495181 -0.07144302  0.09678311 -0.01447989  0.02031577\n",
      "   0.1296505  -0.1610787  -0.1308239   0.03172531  0.04883466  0.10811642\n",
      "  -0.18929039  0.05241186  0.03242867 -0.05898813  0.09444357  0.08690354\n",
      "  -0.02407382 -0.25667083 -0.21501343  0.04346397 -0.04669096 -0.12511373\n",
      "  -0.27539584 -0.17878291 -0.05039798  0.09051374 -0.03547024 -0.00434649\n",
      "   0.06960098  0.20063642  0.30502638 -0.10545105 -0.06673463  0.03741639\n",
      "   0.19309698  0.038365    0.21956594 -0.17646796  0.20492491 -0.10778386\n",
      "   0.23089023  0.13028319 -0.04543047 -0.1204147   0.07122037  0.1217899\n",
      "  -0.21234237 -0.11287967 -0.13965218  0.03537518 -0.06854864 -0.04969741\n",
      "   0.02300574 -0.12516892  0.18406676 -0.00841828  0.27194366  0.10741247\n",
      "   0.06804176 -0.11747518  0.22114083  0.15825044 -0.07426211 -0.09164718\n",
      "  -0.12002245 -0.01233959  0.15168403 -0.0742417  -0.16259927 -0.03927001\n",
      "  -0.11714634  0.09153414  0.16275012 -0.07039782  0.17674161 -0.04670224\n",
      "  -0.08737652 -0.06877595 -0.03194547 -0.07320571  0.0174427   0.22817631\n",
      "  -0.03360892 -0.40593097  0.1935223  -0.1527122  -0.12547557  0.00380012\n",
      "  -0.04200514  0.14323027  0.01994104 -0.08688065 -0.09212688 -0.15325007\n",
      "   0.04403918  0.16649164  0.13607621 -0.13809109 -0.22656074  0.04499033\n",
      "   0.16455436 -0.07329562  0.05772312 -0.27354348  0.0096324   0.04025676\n",
      "  -0.04988051 -0.01495248 -0.07843257 -0.06077591  0.15662076  0.07789534\n",
      "   0.0635252  -0.0777519  -0.08695164 -0.08115426 -0.04695356  0.04679932\n",
      "   0.09050649  0.13786444  0.02024672  0.0247271   0.190444   -0.16340742\n",
      "  -0.04632974  0.13418257 -0.06099918  0.01360317  0.00993459  0.09677276\n",
      "  -0.12845223  0.04579587  0.23956586  0.06339465 -0.0421048   0.18430655\n",
      "  -0.00308657  0.02932196]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ValueException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      5\u001b[0m ds \u001b[38;5;241m=\u001b[39m DATASET(a, smiles_to_bigraph, AttentiveFPAtomFeaturizer(), cache_file_path \u001b[38;5;241m=\u001b[39m cache_path)\n\u001b[1;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset_with_gcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_class_tox21\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtox21_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m      8\u001b[0m     data_ds\u001b[38;5;241m.\u001b[39mappend(d)\n",
      "Cell \u001b[1;32mIn[38], line 20\u001b[0m, in \u001b[0;36mcreate_dataset_with_gcn\u001b[1;34m(dataset, class_embed_vector, GCN, tasks, numberTask)\u001b[0m\n\u001b[0;32m     18\u001b[0m embbed \u001b[38;5;241m=\u001b[39m embbed\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(embbed)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mValueException\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m a \u001b[38;5;241m=\u001b[39m ( embbed, onehot_encoded[numberTask], class_embed_vector[numberTask], label, numberTask, tasks[numberTask])\n\u001b[0;32m     22\u001b[0m created_data\u001b[38;5;241m.\u001b[39mappend(a)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ValueException' is not defined"
     ]
    }
   ],
   "source": [
    "data_ds = []\n",
    "for i, task in  enumerate(tox21_tasks):\n",
    "    a = df[['smiles' , task]]\n",
    "    a = a.dropna()\n",
    "    ds = DATASET(a, smiles_to_bigraph, AttentiveFPAtomFeaturizer(), cache_file_path = cache_path)\n",
    "    data = create_dataset_with_gcn(ds, embed_class_tox21, gcn_model, tox21_tasks, i)\n",
    "    for d in data:\n",
    "        data_ds.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_ds.pkl\", 'wb') as out:\n",
    "    pickle.dump(data_ds, out, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_ds.pkl\", 'rb') as inp:\n",
    "    data_ds = pickle.load(inp)3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.75903082e-01,  1.75110787e-01, -1.10258728e-01,\n",
       "         -6.89792931e-02,  1.15987450e-01, -8.92718136e-02,\n",
       "          4.67717089e-02, -4.96577732e-02,  4.01814669e-01,\n",
       "          5.45490980e-02,  3.02639961e-01,  1.44839242e-01,\n",
       "          2.09039778e-01,  5.83123229e-02,  2.63480753e-01,\n",
       "         -3.92698124e-02, -3.07693463e-02,  2.50586048e-02,\n",
       "         -3.43725719e-02,  5.80183677e-02, -2.42571771e-01,\n",
       "          2.32263982e-01, -7.34976828e-02,  1.59252599e-01,\n",
       "          6.83396906e-02, -4.93671745e-04, -5.20542674e-02,\n",
       "         -1.52324051e-01, -2.02982396e-01, -4.39596623e-01,\n",
       "          1.71046302e-01,  6.83233738e-02,  1.37512386e-01,\n",
       "          7.58431852e-02, -2.07903579e-01, -2.12177619e-01,\n",
       "         -2.73431361e-01,  7.30374306e-02, -4.58746292e-02,\n",
       "          2.52027035e-01,  6.47154376e-02,  1.16163298e-01,\n",
       "         -1.28124475e-01,  1.04325332e-01, -6.96862042e-02,\n",
       "          2.61383951e-01, -6.02222495e-02,  7.98027068e-02,\n",
       "          1.20664775e-01,  1.56286135e-01,  5.31288981e-03,\n",
       "         -7.53875449e-03,  9.23001766e-02,  8.39126557e-02,\n",
       "          3.36023159e-02,  2.16517299e-02, -4.96782325e-02,\n",
       "         -5.84942661e-02,  4.17246133e-01,  3.48867550e-02,\n",
       "         -9.05869305e-02,  2.80822050e-02,  1.40989330e-02,\n",
       "          1.31474480e-01, -5.30792996e-02,  3.65544483e-02,\n",
       "          6.10874444e-02,  1.37918025e-01, -2.00086340e-01,\n",
       "          1.26265496e-01,  6.80196583e-02, -1.13554284e-01,\n",
       "         -7.89835155e-02,  2.20028728e-01,  9.49441791e-02,\n",
       "         -3.12754549e-02,  5.29408008e-02,  2.44839013e-01,\n",
       "          1.16444141e-01,  2.90393651e-01, -5.60236312e-02,\n",
       "         -1.31093338e-01, -1.83433384e-01,  1.03963509e-01,\n",
       "          3.66763115e-01,  6.15782253e-02,  1.93686932e-01,\n",
       "         -1.33517802e-01,  1.52649321e-02,  9.80743766e-02,\n",
       "         -2.38647208e-01,  3.14157784e-01,  1.19619697e-01,\n",
       "          8.16047713e-02, -1.48837104e-01,  8.88602138e-02,\n",
       "          8.24432522e-02,  1.39265835e-01,  2.32300535e-01,\n",
       "         -1.62705928e-02,  2.14219168e-01, -3.16766441e-01,\n",
       "         -6.64568618e-02,  1.25407711e-01,  4.22343090e-02,\n",
       "         -2.45847367e-02, -1.92123540e-02,  9.58348513e-02,\n",
       "          2.22154930e-02,  2.09517807e-01,  8.44217539e-02,\n",
       "          6.70580417e-02, -1.72689572e-01, -1.67383492e-01,\n",
       "         -1.37554407e-01,  4.93926741e-02, -1.16082177e-01,\n",
       "          1.06953397e-01, -1.94366686e-02,  1.11263007e-01,\n",
       "         -6.73957840e-02, -1.07411616e-01, -1.02922231e-01,\n",
       "          1.14928588e-01, -8.71471092e-02, -2.08240360e-01,\n",
       "         -1.72157854e-01, -5.71936406e-02,  3.93638946e-03,\n",
       "          2.96447307e-01, -5.06974161e-02, -9.79209840e-02,\n",
       "          2.04831868e-01,  1.08335033e-01, -6.69342801e-02,\n",
       "          1.28766879e-01, -1.11329809e-01,  8.86769295e-02,\n",
       "         -6.33750111e-02,  1.73237085e-01,  3.29763368e-02,\n",
       "          5.30462712e-02, -6.80168271e-02, -1.26079679e-01,\n",
       "         -1.20375045e-01, -2.35699229e-02, -4.25986461e-02,\n",
       "          6.21494614e-02, -1.17950186e-01,  2.40437351e-02,\n",
       "          3.04930419e-01,  2.12120891e-01, -1.86633825e-01,\n",
       "          1.09787710e-01,  6.22564442e-02, -1.65990308e-01,\n",
       "         -4.41979840e-02,  6.51537627e-03,  8.68498236e-02,\n",
       "         -1.45160049e-01,  5.62298335e-02, -4.03481573e-02,\n",
       "          1.11259550e-01,  3.36186029e-02, -4.40692604e-02,\n",
       "          1.34012565e-01,  8.44020396e-04, -6.28376827e-02,\n",
       "          1.58601657e-01,  2.02909280e-02, -4.27652486e-02,\n",
       "          1.68389469e-01,  1.06678158e-01,  2.49727443e-03,\n",
       "         -2.98539430e-01,  1.65313616e-01, -1.16219953e-01,\n",
       "          2.59668529e-01,  1.68183260e-02,  2.34815344e-01,\n",
       "         -1.18609563e-01, -7.80872405e-02,  7.09158629e-02,\n",
       "          1.33525938e-01, -1.13411635e-01, -3.03159565e-01,\n",
       "          2.56881654e-01, -2.95861423e-01,  3.69966924e-01,\n",
       "          2.18741894e-02, -1.36415452e-01,  1.26605898e-01,\n",
       "         -4.40353528e-02,  2.22252235e-01,  3.84439409e-01,\n",
       "         -2.94421613e-01, -2.14751989e-01, -8.50526094e-02,\n",
       "         -2.10158125e-01,  1.11922622e-01,  1.30198583e-01,\n",
       "         -2.18409374e-01,  1.33624882e-01, -2.49707013e-01,\n",
       "         -1.58049330e-01, -2.51638666e-02,  8.58679041e-03,\n",
       "          2.40310043e-01,  1.11800045e-01, -7.45694786e-02,\n",
       "         -1.23738185e-01,  9.43719298e-02, -1.05635628e-01,\n",
       "          5.07834405e-02, -1.21273801e-01, -6.06986620e-02,\n",
       "          1.37511306e-02,  1.65434822e-01,  2.33337656e-01,\n",
       "          4.01555449e-02, -2.87992716e-01,  1.16936252e-01,\n",
       "          1.46033034e-01, -1.51476279e-01,  2.41844058e-01,\n",
       "         -6.15454130e-02, -2.38532528e-01, -2.46126637e-01,\n",
       "         -7.77663589e-02, -4.75030914e-02, -3.04287337e-02,\n",
       "          5.05851172e-02,  1.42930463e-01,  1.29540548e-01,\n",
       "          6.71418682e-02, -1.17094472e-01, -6.19485900e-02,\n",
       "         -1.51151851e-01, -6.87868595e-02,  1.29656211e-01,\n",
       "         -2.03087091e-01, -8.10887814e-02,  3.53936329e-02,\n",
       "          2.38081083e-01,  1.35921061e-01,  5.09990640e-02,\n",
       "         -4.69248712e-01, -1.21374264e-01, -3.19411978e-03,\n",
       "         -1.78068936e-01,  8.46888572e-02,  4.42471541e-02,\n",
       "         -1.71952322e-01, -2.19050556e-01, -3.45714390e-02,\n",
       "          6.54007569e-02,  2.89483108e-02,  1.09996907e-02,\n",
       "          2.07401261e-01, -1.21582597e-01,  5.32752983e-02,\n",
       "         -1.13598503e-01,  3.65747437e-02, -1.87856629e-01,\n",
       "         -2.80121654e-01,  4.53790314e-02,  7.00721517e-03,\n",
       "          5.73417619e-02,  7.68218338e-02,  1.24401376e-01,\n",
       "         -1.74238428e-01, -2.07499221e-01, -3.21509093e-02,\n",
       "         -2.19274357e-01,  6.68608695e-02, -8.01580101e-02,\n",
       "          2.30835184e-01,  1.60539836e-01,  2.41952837e-01,\n",
       "          9.72770154e-02, -1.80441037e-01,  2.56753825e-02,\n",
       "         -4.67625290e-01,  2.19690800e-01, -4.15312760e-02,\n",
       "         -3.13683935e-02,  3.92308645e-03, -3.80450040e-02,\n",
       "          2.54966646e-01, -1.09938234e-01,  1.32401943e-01,\n",
       "         -1.44407690e-01,  3.18994403e-01,  3.21770221e-01,\n",
       "         -2.28630632e-01, -3.08201294e-02, -2.05457151e-01,\n",
       "          5.10174036e-03,  1.87329188e-01, -2.52567232e-01,\n",
       "         -2.51598477e-01, -1.00891516e-01,  1.35669529e-01,\n",
       "          7.50836581e-02, -2.22454425e-02,  9.32244509e-02,\n",
       "         -6.29747361e-02, -1.12725958e-01, -1.49751514e-01,\n",
       "          2.16039315e-01, -1.25729769e-01,  9.87734869e-02,\n",
       "         -5.49037792e-02, -1.50835603e-01, -9.61204767e-02,\n",
       "         -1.54377908e-01, -1.33797809e-01,  1.27765268e-01,\n",
       "          1.91699341e-01, -8.12779441e-02,  3.18047732e-01,\n",
       "          2.52416909e-01, -1.57690138e-01, -1.08262733e-01,\n",
       "          2.42199332e-01,  9.10404474e-02, -1.00585580e-01,\n",
       "          4.48320806e-03, -8.85768682e-02, -1.05224296e-01,\n",
       "         -1.36543140e-01,  7.19821304e-02,  5.69981895e-02,\n",
       "         -6.61348552e-02, -1.17577627e-01, -2.48073488e-01,\n",
       "          3.56423296e-02,  3.62927854e-01,  1.22878462e-01,\n",
       "          8.50368440e-02, -8.66089761e-02,  8.89823958e-03,\n",
       "          7.28759021e-02, -3.19938391e-01,  7.94168934e-02,\n",
       "         -4.50973362e-01, -2.11671576e-01,  2.56032925e-02,\n",
       "         -1.07927203e-01,  1.84119552e-01,  4.77655269e-02,\n",
       "          1.39980122e-01,  1.93876520e-01,  1.14797875e-01,\n",
       "          5.35967946e-02, -2.64070570e-01, -1.32901222e-01,\n",
       "         -3.20990324e-01, -2.86541879e-01, -3.92522849e-03,\n",
       "         -1.27088115e-01,  7.30499998e-03,  1.59127265e-03,\n",
       "          3.59106176e-02, -1.66346937e-01, -1.38283312e-01,\n",
       "          5.75776771e-03, -2.58476704e-01, -1.76866144e-01,\n",
       "         -2.31838644e-01, -2.87632216e-02,  6.62587136e-02,\n",
       "         -8.80147517e-02, -3.54041569e-02,  9.88759547e-02,\n",
       "         -1.12183228e-01, -2.55836666e-01,  1.77318007e-02,\n",
       "          2.35912099e-01, -1.04820810e-01, -1.37541890e-02,\n",
       "          3.31644937e-02,  1.89929485e-01, -1.60223916e-01,\n",
       "          6.56869709e-02,  1.01475202e-01, -6.90436363e-03,\n",
       "          1.88775972e-01, -1.80668831e-01,  3.09490919e-01,\n",
       "         -9.88679007e-03,  5.55441901e-02,  1.48180485e-01,\n",
       "         -1.16759557e-02,  1.34353369e-01,  1.33430541e-01,\n",
       "         -1.58646852e-01,  9.99697372e-02, -2.06484362e-01,\n",
       "          2.48749964e-02,  2.17354119e-01, -2.38421224e-02,\n",
       "          1.82996497e-01, -5.07423319e-02, -8.53342190e-03,\n",
       "          7.42825866e-02, -3.44459787e-02, -7.50446171e-02,\n",
       "          1.33953556e-01,  1.20037898e-01, -1.26754358e-01,\n",
       "         -1.56099610e-02, -1.08851381e-02, -1.90146476e-01,\n",
       "          8.95369053e-02, -1.79151803e-01,  1.07897326e-01,\n",
       "         -3.20584893e-01, -9.13347825e-02,  1.22974068e-02,\n",
       "          1.25785679e-01,  1.09298006e-01, -1.27035484e-01,\n",
       "         -1.80275161e-02,  1.05184279e-02,  2.03398779e-01,\n",
       "          9.38484818e-02,  1.26935452e-01, -3.44636440e-02,\n",
       "          1.56139717e-01, -4.52892557e-02, -7.71558732e-02,\n",
       "          1.42032281e-03,  7.46587664e-02,  8.37179720e-02,\n",
       "         -1.80334434e-01, -1.36987776e-01,  5.18594272e-02,\n",
       "          1.05905607e-01,  1.99193954e-02, -1.03400044e-01,\n",
       "          7.93189183e-03, -5.29133156e-03, -3.53955291e-02,\n",
       "         -8.68766233e-02,  9.10993889e-02,  2.43311718e-01,\n",
       "         -9.76908803e-02,  1.04596257e-01, -3.02282535e-03,\n",
       "          5.95726036e-02,  1.95130110e-01, -1.69545025e-01,\n",
       "         -1.31875649e-01,  9.20923129e-02, -8.54171365e-02,\n",
       "         -4.20858189e-02,  9.63360444e-02, -6.00937381e-03,\n",
       "          2.00152278e-01,  1.67470723e-01,  1.90975070e-01,\n",
       "          2.20756203e-01,  8.43993127e-02,  1.35530144e-01,\n",
       "          9.30264145e-02,  1.89638272e-01, -1.38777018e-01,\n",
       "         -6.09031282e-02, -5.55255525e-02, -5.19450754e-02,\n",
       "         -1.31549731e-01,  3.53374600e-01,  3.59437466e-02,\n",
       "          7.33885318e-02, -7.09371418e-02,  1.09029964e-01,\n",
       "          2.28654772e-01, -5.78600168e-02, -9.75901186e-02,\n",
       "          2.23433062e-01,  1.05963629e-02, -1.74637094e-01,\n",
       "         -3.99950892e-04,  2.56119490e-01,  3.02375667e-03,\n",
       "          1.44730404e-01,  1.43602848e-01, -4.15610783e-02,\n",
       "          8.20111483e-02,  3.64236444e-01, -1.32457837e-01,\n",
       "          1.58869565e-01, -8.61561373e-02, -2.18630329e-01,\n",
       "         -2.78135128e-02,  7.69522190e-02, -6.30651116e-02,\n",
       "         -3.39212924e-01, -1.67029530e-01, -1.33923870e-02,\n",
       "          2.05481455e-01,  3.18793118e-01,  5.16439490e-02,\n",
       "          4.28426042e-02, -2.04055812e-02, -3.09529781e-01,\n",
       "          1.73176318e-01,  8.85583088e-02,  2.31080845e-01,\n",
       "         -7.14497045e-02,  7.50731230e-02]], dtype=float32),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([-3.53499729e-01, -4.50154189e-01, -1.89145125e-01,  9.80319232e-01,\n",
       "        -1.44063644e-01,  1.96180889e-01,  2.29582047e-02,  2.89855391e+00,\n",
       "         1.96210651e-01,  3.67702189e-01,  5.22396211e-01,  8.90701690e-01,\n",
       "         4.95457019e-03, -1.06179296e-01,  2.64723994e-02, -8.54007235e-02,\n",
       "         1.44865130e+00,  3.22570170e+00,  1.40330841e+00, -6.83409331e-01,\n",
       "         1.68633563e+00,  1.56463924e-01,  1.58416273e+00, -6.86314048e-02,\n",
       "        -3.45267271e-01, -1.17627883e-02,  3.41304173e-01,  5.80078826e-01,\n",
       "         6.06739707e-01, -3.05930930e-01,  3.82035056e-02, -4.51846347e-01,\n",
       "         1.24194465e+00,  1.97522893e-01, -4.28855044e-01,  2.03955789e+00,\n",
       "         1.49616829e+00,  5.12457645e-01, -6.83409331e-01,  4.41676192e-02,\n",
       "        -1.03897719e+00,  2.28221205e-01, -3.80080086e-01,  1.79026184e+00,\n",
       "        -7.82471601e-01, -5.99398685e-01,  1.50598877e-01,  1.19681049e+00,\n",
       "        -1.21454435e+00, -3.10753106e-03,  1.34668180e+00,  1.31467291e+00,\n",
       "        -8.92858517e-01,  5.65830815e-01, -1.41955990e-02, -5.40529309e-01,\n",
       "        -8.12463419e-01,  1.79763432e+00, -4.76128190e-01,  9.46213459e-01,\n",
       "         7.93006738e-03,  1.19203723e+00,  6.65270992e-01, -2.07887984e-01,\n",
       "        -4.31609319e-01, -3.35858765e-01,  2.55023745e-02,  2.60056215e-02,\n",
       "         1.30259755e+00, -6.86314048e-02,  4.51283902e-01,  6.04170656e-01,\n",
       "        -1.13669952e-01,  4.84795866e-01, -2.77053676e-01,  8.65033857e-01,\n",
       "         1.33352216e+00, -4.85079048e-01,  8.85331955e-01, -2.82699069e-01,\n",
       "        -1.95302306e-02,  1.21667611e+00,  2.73357781e-01,  2.39527703e-01,\n",
       "         2.90908511e+00, -1.31132340e-01, -8.62035153e-02,  2.74182262e-01,\n",
       "         1.90974955e+00, -7.56772437e-01, -3.85077606e-01, -5.93091697e-01,\n",
       "         6.36466566e-01,  1.39176484e-02, -3.75905127e-01, -3.23885899e-01,\n",
       "         5.01095821e-01, -2.60832034e-01,  4.37743774e-01,  8.50335558e-01,\n",
       "        -4.40962250e-01,  1.32612463e-01,  6.40362170e-01, -1.81647194e-01,\n",
       "         1.78339976e-01,  1.87521768e+00,  2.54337143e-01, -1.37231380e+00,\n",
       "        -5.55603603e-01, -1.91819573e-01,  2.23660676e+00, -7.04088699e-02,\n",
       "         2.19899312e-01, -3.78011619e-01, -6.29525611e-01,  8.38191832e-01,\n",
       "        -6.08255545e-01,  4.28349455e-01,  6.39833533e-01, -1.27258513e-01,\n",
       "         8.92155272e-01, -1.00219825e-01,  1.02017459e+00, -1.24599523e-01,\n",
       "         1.15358856e+00,  3.68131425e-03,  5.88083795e-01,  1.88818620e+00,\n",
       "         1.12218700e-01,  8.65702454e-01,  1.83446069e+00, -1.31111254e-01,\n",
       "        -3.82292641e-01, -3.22742666e-02, -5.93750604e-02, -2.72684697e-01,\n",
       "         1.41597532e-01,  7.78441426e-01,  2.82149422e-01, -2.96600880e-01,\n",
       "        -7.37766123e-01,  8.82077546e-01, -3.43649492e-01,  1.00499457e+00,\n",
       "         3.58924928e-01, -1.81781582e-01, -1.67772369e-01, -1.06406391e-01,\n",
       "        -1.53537446e-01,  4.85862399e-01, -6.99505868e-01,  7.62390946e-01,\n",
       "         9.42899277e-03,  6.28865988e-01,  3.86020185e-01,  4.11253126e-01,\n",
       "        -4.69211077e-01,  1.25382689e-02, -1.08500475e+00,  1.09834309e+00,\n",
       "        -8.78301147e-01, -3.12972851e-01,  9.84709786e-01, -1.06839574e-01,\n",
       "         8.75131405e-01, -2.55386559e-01,  1.39176484e-02,  1.72060688e+00,\n",
       "         5.19750631e-01,  7.15661688e-01,  2.18330473e+00, -6.31395480e-01,\n",
       "        -4.08529151e-01, -1.60201592e+00,  1.56376771e-01, -1.92555290e-01,\n",
       "        -1.59254433e-01, -1.96414214e-01,  3.86020185e-01, -5.57906941e-01,\n",
       "        -2.64626103e-01,  6.15172703e-01,  9.72191281e-01,  2.19503814e-01,\n",
       "         7.24662455e-01,  7.93553248e-03, -1.49547186e-01, -4.87716970e-01,\n",
       "         1.38443150e-01,  4.13227681e-01, -4.12192821e-01,  4.93580364e-01,\n",
       "        -2.50075360e-01,  2.26139646e-01,  8.11240254e-01,  1.34443983e+00,\n",
       "        -1.04386397e+00, -2.70310676e-01,  2.23587284e-01,  1.30212556e+00,\n",
       "         1.13717338e-01,  3.28083058e-01,  1.44216008e+00, -2.63650635e-01,\n",
       "        -8.05630761e-01,  1.13413675e+00,  4.46746882e-01, -8.97104472e-02,\n",
       "        -1.07461917e+00, -2.31031371e-01,  5.06116349e-02,  2.32168466e-01,\n",
       "         6.65017106e-01, -2.15155437e-01, -3.29975864e-01,  1.35018993e-01,\n",
       "        -8.32848176e-02,  5.01860472e-01, -3.73308346e-01, -3.51651870e-01,\n",
       "        -9.19293618e-01,  2.01598914e-01,  4.41215937e-01,  4.16875298e-01,\n",
       "        -8.00550505e-03,  2.65394537e-01,  1.14691936e+00, -2.26463841e-01,\n",
       "         3.69068212e-01, -5.07406206e-01,  3.10470706e-01,  7.45875560e-01,\n",
       "        -3.07695773e-01,  1.09847110e+00, -8.92402943e-01, -7.62881594e-01,\n",
       "         1.91678755e+00, -1.31142886e-01,  1.33527656e+00,  9.54328724e-01,\n",
       "         1.41252948e+00,  9.56153828e-02,  1.70537539e+00, -2.43767575e-01,\n",
       "        -1.59305284e+00,  6.60740101e-01,  5.36147526e-01, -2.06145765e-01,\n",
       "         2.15111173e+00,  6.75791762e-01,  1.69320471e+00,  1.05671222e+00,\n",
       "        -6.28113427e-01,  1.67839541e+00,  1.42187973e+00,  9.22380397e-01,\n",
       "        -5.55603603e-01,  1.09927679e-01,  6.63856928e-01, -5.14549804e-01,\n",
       "        -6.75457488e-01,  1.23725702e+00,  1.86486390e+00, -3.51625703e-01,\n",
       "         6.35331830e-01,  4.22572373e-01, -2.67843703e-01,  4.30358420e-01,\n",
       "         1.20153902e+00,  1.27551126e+00,  7.30406414e-01, -2.66569782e-03,\n",
       "         9.76406266e-02, -1.47529815e-01,  5.31639220e-01,  1.98613303e+00,\n",
       "        -1.87883923e-01, -1.27341176e-02, -3.49644630e-02,  1.10315503e+00,\n",
       "         1.66923430e-01,  1.08755911e+00, -2.49309078e-01,  1.07117232e-01,\n",
       "        -4.75524283e-01,  8.29691864e-01,  7.83087775e-01, -3.75624675e-01,\n",
       "        -5.00579055e-01,  7.13649150e-01, -4.65099473e-02,  3.83868986e-01,\n",
       "         1.92498677e+00, -1.69315536e-01, -1.99624281e-01,  2.76180305e-01,\n",
       "         8.17458836e-01, -6.15446184e-01,  1.87173185e-01,  4.00736126e-01,\n",
       "         4.86614606e-01,  1.49574690e+00,  1.49972525e-01,  4.92815754e-01,\n",
       "        -3.79242743e-01, -7.77384333e-01,  1.82670011e-01, -1.00165337e+00,\n",
       "         8.64293104e-02,  2.29641818e-02, -3.87444049e-01,  5.93001463e-01,\n",
       "        -3.19757215e-01,  3.80540514e-01,  1.18828482e+00,  5.33692807e-01,\n",
       "        -6.11321575e-01, -5.00364225e-02,  2.43978090e-01, -2.78787040e-01,\n",
       "         4.33697988e-02, -3.10805692e-02, -1.99194390e-02,  7.41404422e-01,\n",
       "         6.07493446e-01, -2.42401318e-01, -4.34630442e-01,  4.26568070e-01,\n",
       "        -1.00707140e+00,  7.05321272e-01, -6.40034260e-01,  5.61360218e-01,\n",
       "        -3.35858765e-01,  2.00594452e-01,  8.24983641e-01, -2.81481398e-01,\n",
       "        -2.86986080e-01,  2.71842782e-01,  7.47745559e-02, -1.61089937e+00,\n",
       "         6.28942829e-01, -6.97397630e-01,  3.67584913e-01, -5.30876420e-01,\n",
       "        -8.07099406e-01,  2.77292935e-01, -2.80338723e-01, -4.69211077e-01,\n",
       "         1.85256160e+00,  5.89489001e-02, -3.79242743e-01, -5.23252886e-01,\n",
       "         7.42575300e-02, -2.03683296e-01,  2.64071495e-01,  5.03030240e-04,\n",
       "         2.60557084e-01,  7.81690281e-02, -4.04704744e-01,  8.66455896e-01,\n",
       "        -1.10154901e-01, -1.42688694e-01,  8.38992752e-01,  2.12932380e-02,\n",
       "         6.46929657e-01, -1.72393555e-01, -7.12678329e-01, -2.57284587e-01,\n",
       "         3.41290212e-01, -4.50761490e-01,  2.46994968e-01,  5.65764049e-02,\n",
       "         2.90570721e-01,  1.63959080e+00,  2.05219490e-01, -2.00577090e-01,\n",
       "         2.53275731e-01,  7.90147984e-02, -4.27974369e-01, -2.75881325e-01,\n",
       "         5.03094997e-04,  3.90008509e-01, -3.22620091e-02,  2.62796110e-01,\n",
       "        -1.29958966e-01,  5.75964819e-01, -5.56586178e-02, -2.75943390e-01,\n",
       "         6.77474322e-01, -1.91913662e-01,  6.80104206e-01, -9.28409900e-01,\n",
       "         2.46827726e-01,  1.73055641e-01, -2.26996424e-01,  2.80471732e-01,\n",
       "         4.05839495e-01,  1.03431023e+00,  4.05891046e-01, -4.29601668e-02,\n",
       "        -4.33889078e-01, -6.92503537e-01,  1.32860497e+00,  1.35163835e+00,\n",
       "         1.64352031e+00,  8.11191956e-01,  6.13700745e-01,  1.84686653e-01,\n",
       "         2.63653797e+00,  1.04053282e-01, -4.63700014e-01,  4.05710675e-01,\n",
       "         2.50098710e+00,  3.14833264e-01,  8.59374220e-01,  9.16688605e-01,\n",
       "         4.26918957e-01, -5.05663604e-01,  2.00018595e-01, -2.71844449e-01,\n",
       "        -5.36581320e-01, -8.17001786e-02,  3.00487530e-01, -2.85239108e-01,\n",
       "         1.61547854e+00,  2.70079751e-01,  2.86602416e-01, -9.61159453e-01,\n",
       "        -1.93056514e-01,  1.20769623e-01,  1.93176198e-01, -5.23252886e-01,\n",
       "         1.39914675e-01, -8.29421430e-02,  5.13405814e-02,  7.33431766e-01,\n",
       "         6.66689562e-01,  5.16484724e-01, -4.60008074e-01,  1.89818100e+00,\n",
       "        -8.27128620e-01,  1.23278596e-01, -9.88139961e-01,  1.84641904e-01,\n",
       "         4.38148352e-01, -3.63951306e-01, -2.40179397e-01, -9.81201074e-02,\n",
       "         1.23541996e-01,  6.05951491e-02,  1.48963750e+00, -7.03433626e-01,\n",
       "         1.21309152e-01,  1.71367056e+00, -1.04805985e-01,  1.52299807e+00,\n",
       "        -3.73899587e-01, -6.64098219e-01,  3.15394084e-01, -2.44451654e-01,\n",
       "         1.62884742e+00,  1.93741915e+00,  6.98000813e-01, -1.76222260e-01,\n",
       "        -2.74507285e-01,  1.90552664e+00, -3.01248606e-01, -1.75096555e+00,\n",
       "         1.26051529e+00, -1.59299909e-01,  1.35043801e-01, -1.04805985e-01,\n",
       "         1.64999587e-02, -5.46845501e-01,  1.60756120e-01,  4.74790552e-01,\n",
       "         6.28811356e-01,  1.87173185e-01,  5.65522284e-01,  1.79462288e+00,\n",
       "        -9.13892281e-02, -3.42620587e-01, -1.22843260e-01,  1.59001808e-01,\n",
       "         9.76406266e-02,  1.83377484e+00, -5.49288074e-02,  1.46403070e-01,\n",
       "        -5.49107931e-01, -6.23364394e-01,  1.40193774e+00, -3.73655634e-01,\n",
       "        -2.89346784e-01,  9.07761614e-01,  1.04574651e+00, -1.15462136e+00,\n",
       "        -1.26063200e+00,  7.84665733e-01, -9.41633629e-01, -1.29502923e-01,\n",
       "        -9.19757578e-01, -1.13669952e-01, -1.75016384e-01,  2.64004822e-01,\n",
       "         2.05001498e+00,  4.29828148e-01, -3.76216164e-01,  1.02680639e+00,\n",
       "         9.00039170e-01, -1.43974360e-01, -3.22742666e-02, -3.00343847e-01]),\n",
       " tensor([0.]),\n",
       " 0,\n",
       " 'NR-AR')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9wZRIKrq2Kec",
    "outputId": "6a5e45aa-32cb-47da-d25c-325f6cfe106b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-9.29306448e-02,  1.11635879e-01, -1.00457221e-01,\n",
      "         1.62047401e-01,  1.24492869e-02,  1.08428285e-01,\n",
      "         2.22544372e-02,  2.18045432e-02,  1.21816851e-01,\n",
      "         1.39299054e-02, -3.63279879e-02,  8.56475383e-02,\n",
      "         2.15980679e-01,  1.99645936e-01,  1.55550882e-01,\n",
      "        -6.36141673e-02, -1.49515212e-01, -7.06169382e-03,\n",
      "         4.60457169e-02, -3.30150388e-02,  4.41604480e-02,\n",
      "         7.06224293e-02,  6.61159456e-02,  1.46019280e-01,\n",
      "         7.45667368e-02, -7.04252124e-02,  5.15302010e-02,\n",
      "        -3.32872085e-02, -6.21295720e-02, -1.69580564e-01,\n",
      "         1.20833486e-01,  7.19942451e-02,  5.62678948e-02,\n",
      "         8.65218043e-02, -1.61455981e-02, -4.96086143e-02,\n",
      "        -3.71877849e-02, -3.08133028e-02,  3.17478776e-02,\n",
      "         9.73401666e-02,  2.07846895e-01, -1.60953961e-02,\n",
      "        -3.69034111e-02, -2.15739384e-02,  6.61027879e-02,\n",
      "         7.91918337e-02, -4.24526818e-02,  7.76807666e-02,\n",
      "         2.24440590e-01,  6.53573871e-02,  2.78429538e-02,\n",
      "         9.71274525e-02, -1.60738453e-02,  5.22590987e-02,\n",
      "         7.96958655e-02, -4.40078340e-02, -2.22258538e-01,\n",
      "        -3.80993262e-02,  3.88432443e-02, -1.22028813e-01,\n",
      "        -2.72979178e-02,  7.50141069e-02, -9.11414996e-03,\n",
      "        -1.24341622e-02, -1.26394421e-01, -9.57815200e-02,\n",
      "        -9.35845226e-02,  6.11819662e-02, -1.17083669e-01,\n",
      "         8.28661993e-02,  6.19259700e-02, -6.84394538e-02,\n",
      "         2.14877818e-02,  5.12485169e-02,  1.71144493e-02,\n",
      "        -1.38996243e-01,  4.18552682e-02,  1.69090092e-01,\n",
      "        -1.18304603e-02, -1.48929916e-02, -3.71719748e-02,\n",
      "        -2.22062469e-01, -8.54535848e-02, -9.57995504e-02,\n",
      "         4.38036583e-02, -5.02267107e-02, -1.15634799e-02,\n",
      "        -8.39773118e-02, -3.92799526e-02,  5.62300198e-02,\n",
      "        -9.46965069e-02,  4.85797711e-02, -1.82105407e-01,\n",
      "        -9.78724435e-02, -1.43700913e-02, -5.19153848e-03,\n",
      "         1.28740594e-01, -2.85202153e-02,  1.80092543e-01,\n",
      "         3.97022255e-03, -2.46947147e-02, -1.84202641e-01,\n",
      "        -1.31890565e-01,  1.03901148e-01,  1.60205215e-01,\n",
      "         1.10550031e-01,  1.45811141e-01,  1.43252760e-02,\n",
      "         6.75090551e-02,  2.04429016e-01,  1.08364727e-02,\n",
      "         2.50521712e-02, -4.36541773e-02, -1.47179067e-01,\n",
      "        -8.18971097e-02,  6.09545074e-02,  4.06049192e-04,\n",
      "         7.28543103e-02, -2.02116556e-02, -1.23505481e-02,\n",
      "        -1.34020656e-01, -1.11449666e-01, -4.91927192e-02,\n",
      "        -1.20945722e-01,  8.37689117e-02,  4.96764854e-03,\n",
      "        -1.01288855e-01,  8.48967955e-03,  5.33321351e-02,\n",
      "         1.94142222e-01,  5.58166802e-02, -1.84239924e-01,\n",
      "         2.44669944e-01,  1.49418131e-01, -2.88092606e-02,\n",
      "         1.93119511e-01, -1.33119062e-01,  6.73925653e-02,\n",
      "        -2.43849382e-02,  1.24887720e-01, -1.99802555e-02,\n",
      "         9.14505869e-02,  4.41373363e-02, -8.28924924e-02,\n",
      "         2.67543122e-02,  2.81773694e-02,  4.82308492e-02,\n",
      "        -1.62806995e-02,  3.47182155e-04,  6.30594343e-02,\n",
      "         1.89602554e-01, -2.18217820e-03,  3.69419530e-03,\n",
      "        -3.79299968e-02,  2.93635353e-02, -7.60392547e-02,\n",
      "         4.55132052e-02,  1.66161478e-01,  7.53934830e-02,\n",
      "        -1.64859965e-01, -1.05225109e-02, -1.61562532e-01,\n",
      "         7.46996179e-02, -1.59474477e-01,  3.79017740e-02,\n",
      "         2.79967748e-02, -1.72071844e-01, -2.32668966e-02,\n",
      "         3.85150313e-04, -1.01620734e-01, -1.48967430e-01,\n",
      "         1.27764642e-01,  1.79093704e-01,  1.65600292e-02,\n",
      "        -2.04105638e-02, -1.00749224e-01,  1.56620815e-02,\n",
      "         6.66309223e-02, -5.37398420e-02,  3.76189686e-02,\n",
      "        -2.26313286e-02,  3.87864709e-02, -5.00077941e-02,\n",
      "         8.82251933e-03, -1.66679621e-01,  5.07474802e-02,\n",
      "         1.93046957e-01,  1.32176876e-02,  1.62221819e-01,\n",
      "         1.54777095e-02,  1.57085598e-01, -6.27363026e-02,\n",
      "         4.61909324e-02, -1.42474279e-01,  2.49676660e-01,\n",
      "        -8.48942176e-02, -3.13819870e-02,  1.68056227e-02,\n",
      "        -1.19154915e-01,  8.07659924e-02,  1.73113316e-01,\n",
      "        -1.24801785e-01,  1.38726160e-01, -4.86936644e-02,\n",
      "         1.15969911e-01, -1.30815357e-01, -6.33012056e-02,\n",
      "        -2.48648170e-02,  9.00128335e-02,  7.91776925e-04,\n",
      "        -2.07681991e-02,  2.33517326e-02,  1.09166294e-01,\n",
      "         9.47163403e-02,  1.69755042e-01, -1.15480661e-01,\n",
      "        -5.08517958e-02,  1.23856321e-01,  6.85469620e-03,\n",
      "        -8.45704228e-02, -9.02486295e-02,  1.58556983e-01,\n",
      "         1.39878049e-01, -2.04153918e-02, -3.31471041e-02,\n",
      "        -9.90731716e-02, -5.72703630e-02, -1.26256067e-02,\n",
      "        -3.04224081e-02, -1.87367816e-02, -7.78534263e-02,\n",
      "        -4.66203280e-02,  1.84705764e-01, -6.26331046e-02,\n",
      "        -1.66255563e-01,  4.01643813e-02, -1.59882724e-01,\n",
      "        -9.21181291e-02, -1.53341725e-01,  5.36193326e-02,\n",
      "        -7.22232461e-03,  1.05511591e-01,  1.83172330e-01,\n",
      "         9.04792696e-02, -7.19062686e-02, -2.04795040e-02,\n",
      "        -8.65608454e-02,  6.81975633e-02,  1.80074796e-01,\n",
      "         6.67986274e-02,  1.79500505e-03,  5.04174270e-02,\n",
      "        -3.64634767e-02, -2.00314820e-01, -1.48445442e-02,\n",
      "         1.39330298e-01,  6.06503002e-02, -1.69854775e-01,\n",
      "         2.04408988e-01, -1.95072554e-02,  1.08610988e-01,\n",
      "        -7.40224570e-02, -1.95391327e-02, -1.14315994e-01,\n",
      "        -7.20638633e-02,  2.68756039e-02,  8.52431655e-02,\n",
      "         1.68452337e-01,  2.10913271e-01,  7.77658820e-02,\n",
      "         3.12832072e-02, -1.55843943e-01, -5.05075455e-02,\n",
      "        -1.21833071e-01, -2.07257010e-02, -1.97931379e-03,\n",
      "         2.38349915e-01,  3.78049165e-03,  1.92253828e-01,\n",
      "         7.24135414e-02, -1.59221411e-01,  4.98448610e-02,\n",
      "        -1.24198079e-01,  1.64368510e-01, -1.14286281e-02,\n",
      "         2.82184854e-02, -4.17496935e-02, -9.44812000e-02,\n",
      "         4.32088785e-02,  7.34368414e-02,  8.08367431e-02,\n",
      "        -7.05090612e-02,  2.76472047e-02,  1.49860546e-01,\n",
      "        -6.51822388e-02, -8.03575143e-02,  2.34982744e-03,\n",
      "        -1.08494341e-01,  6.63309023e-02, -1.22163780e-01,\n",
      "        -1.29589036e-01, -3.14894132e-02, -9.14314613e-02,\n",
      "         4.28741090e-02, -8.53209645e-02,  9.22958851e-02,\n",
      "        -1.39060467e-02,  8.91036540e-03, -8.93929824e-02,\n",
      "         7.78961927e-02,  2.96588652e-02,  1.51591524e-01,\n",
      "        -1.44375145e-01,  1.87375825e-02, -6.44289628e-02,\n",
      "        -1.04028732e-01, -6.45481274e-02,  6.12505898e-03,\n",
      "         4.79240976e-02, -1.16526172e-01,  1.71683818e-01,\n",
      "         1.22958615e-01, -3.96704115e-02,  1.51726991e-01,\n",
      "         2.55383044e-01,  8.38532448e-02,  1.55647099e-03,\n",
      "         6.09588623e-03, -1.14464395e-01,  4.16960157e-02,\n",
      "        -8.59939754e-02,  1.55462861e-01,  6.75860494e-02,\n",
      "         1.08233705e-01,  5.95703907e-03, -1.92451596e-01,\n",
      "        -6.40785694e-02, -1.79760158e-04,  1.07243821e-01,\n",
      "         8.77571851e-02,  2.31050067e-02,  3.26394774e-02,\n",
      "        -1.31417617e-01,  4.29376252e-02,  8.42985585e-02,\n",
      "        -1.02645203e-01, -9.07445624e-02, -1.03326328e-02,\n",
      "         2.79721543e-02,  4.99198288e-02,  6.17669113e-02,\n",
      "         1.61251292e-01,  7.27600157e-02, -1.31405696e-01,\n",
      "         7.95025900e-02, -3.46182659e-02, -1.05533183e-01,\n",
      "        -6.74583763e-02, -1.16646126e-01, -3.84934731e-02,\n",
      "        -3.22249942e-02, -3.56854349e-02,  2.85736173e-02,\n",
      "         4.14247252e-02, -3.99270430e-02, -9.35675949e-02,\n",
      "        -1.20734170e-01, -5.28266467e-02, -1.31321445e-01,\n",
      "         2.73060352e-02,  1.22037917e-01,  4.93516847e-02,\n",
      "        -4.89586182e-02,  1.89338490e-01, -1.15332231e-01,\n",
      "        -6.70402572e-02, -8.71784389e-02,  4.50006612e-02,\n",
      "         1.17807671e-01, -7.10565597e-02, -1.11378722e-01,\n",
      "        -2.11696886e-02,  1.16395518e-01, -3.27755548e-02,\n",
      "        -2.72122547e-02,  7.67443404e-02,  5.84685653e-02,\n",
      "         1.84154391e-01,  9.50632393e-02,  1.84594691e-01,\n",
      "        -5.99492006e-02,  1.65348381e-01,  1.66651398e-01,\n",
      "         1.01789720e-02,  6.42966107e-02,  1.28314853e-01,\n",
      "        -6.46896958e-02,  1.64523460e-02, -1.76790923e-01,\n",
      "         1.14966951e-01,  9.89065617e-02, -6.29138947e-02,\n",
      "        -2.68236175e-03, -9.87755358e-02, -9.67309102e-02,\n",
      "         3.29263471e-02,  7.45510906e-02,  2.05580331e-02,\n",
      "         3.86908725e-02,  1.00046784e-01, -2.51179397e-01,\n",
      "        -1.05602890e-02,  6.78851604e-02, -1.09837294e-01,\n",
      "         1.28507987e-01, -3.22361290e-03,  1.86589174e-02,\n",
      "        -1.43468648e-01, -1.10170357e-01,  5.30657396e-02,\n",
      "        -6.37079179e-02, -6.53859898e-02, -2.19693221e-02,\n",
      "        -1.39634669e-01, -1.36172958e-02,  8.44795257e-02,\n",
      "        -8.95377845e-02, -1.14657022e-01, -1.26497522e-02,\n",
      "         1.04012296e-01,  3.98825780e-02,  3.00623216e-02,\n",
      "        -6.30549043e-02,  4.60169241e-02,  7.03134909e-02,\n",
      "         9.15494859e-02, -8.07026029e-02,  2.43726969e-02,\n",
      "         6.03841171e-02, -6.66396320e-02, -5.97347543e-02,\n",
      "         1.43695608e-01,  1.97125599e-03, -4.66810055e-02,\n",
      "         1.59525067e-01, -1.54233694e-01,  6.65243641e-02,\n",
      "         8.16464052e-02, -1.67893581e-02, -6.26178831e-02,\n",
      "         1.13015682e-01, -4.93249670e-02, -2.72012316e-03,\n",
      "         9.49702039e-03, -9.36432779e-02,  6.18977733e-02,\n",
      "        -1.82681903e-02,  1.00676283e-01, -1.16306365e-01,\n",
      "         3.44796143e-02, -7.59097263e-02,  2.11245455e-02,\n",
      "        -3.89689729e-02, -1.04361683e-01, -3.11187170e-02,\n",
      "        -6.74632005e-03,  1.67443663e-01,  1.27131671e-01,\n",
      "        -4.27540764e-03,  3.72443348e-04,  2.22600382e-02,\n",
      "        -7.12509006e-02,  2.34652460e-01, -8.51456746e-02,\n",
      "        -4.46268916e-03, -1.29879206e-01, -1.04714632e-01,\n",
      "         1.07174665e-01, -4.58181649e-02, -3.64484154e-02,\n",
      "         1.12608373e-01, -1.80105567e-01, -1.38787292e-02,\n",
      "        -3.11277919e-02,  1.01739690e-01, -4.68394160e-02,\n",
      "         8.02268684e-02,  1.89528316e-01, -2.26754338e-01,\n",
      "         4.05928120e-03,  8.23582709e-02, -4.58064936e-02,\n",
      "        -2.95997933e-02,  3.54861021e-02, -1.27139017e-01,\n",
      "        -7.51048028e-02,  1.18764356e-01, -2.87700742e-02,\n",
      "        -1.02402039e-01, -2.80111935e-02, -1.20760381e-01,\n",
      "         1.36339068e-01,  7.41713792e-02, -8.91880393e-02,\n",
      "         4.80248630e-02, -1.06124096e-01, -2.23987922e-01,\n",
      "         1.42370701e-01,  8.91475230e-02,  3.79788950e-02,\n",
      "        -9.26163793e-02, -8.38109851e-02]], dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([-3.53499729e-01, -4.50154189e-01, -1.89145125e-01,  9.80319232e-01,\n",
      "       -1.44063644e-01,  1.96180889e-01,  2.29582047e-02,  2.89855391e+00,\n",
      "        1.96210651e-01,  3.67702189e-01,  5.22396211e-01,  8.90701690e-01,\n",
      "        4.95457019e-03, -1.06179296e-01,  2.64723994e-02, -8.54007235e-02,\n",
      "        1.44865130e+00,  3.22570170e+00,  1.40330841e+00, -6.83409331e-01,\n",
      "        1.68633563e+00,  1.56463924e-01,  1.58416273e+00, -6.86314048e-02,\n",
      "       -3.45267271e-01, -1.17627883e-02,  3.41304173e-01,  5.80078826e-01,\n",
      "        6.06739707e-01, -3.05930930e-01,  3.82035056e-02, -4.51846347e-01,\n",
      "        1.24194465e+00,  1.97522893e-01, -4.28855044e-01,  2.03955789e+00,\n",
      "        1.49616829e+00,  5.12457645e-01, -6.83409331e-01,  4.41676192e-02,\n",
      "       -1.03897719e+00,  2.28221205e-01, -3.80080086e-01,  1.79026184e+00,\n",
      "       -7.82471601e-01, -5.99398685e-01,  1.50598877e-01,  1.19681049e+00,\n",
      "       -1.21454435e+00, -3.10753106e-03,  1.34668180e+00,  1.31467291e+00,\n",
      "       -8.92858517e-01,  5.65830815e-01, -1.41955990e-02, -5.40529309e-01,\n",
      "       -8.12463419e-01,  1.79763432e+00, -4.76128190e-01,  9.46213459e-01,\n",
      "        7.93006738e-03,  1.19203723e+00,  6.65270992e-01, -2.07887984e-01,\n",
      "       -4.31609319e-01, -3.35858765e-01,  2.55023745e-02,  2.60056215e-02,\n",
      "        1.30259755e+00, -6.86314048e-02,  4.51283902e-01,  6.04170656e-01,\n",
      "       -1.13669952e-01,  4.84795866e-01, -2.77053676e-01,  8.65033857e-01,\n",
      "        1.33352216e+00, -4.85079048e-01,  8.85331955e-01, -2.82699069e-01,\n",
      "       -1.95302306e-02,  1.21667611e+00,  2.73357781e-01,  2.39527703e-01,\n",
      "        2.90908511e+00, -1.31132340e-01, -8.62035153e-02,  2.74182262e-01,\n",
      "        1.90974955e+00, -7.56772437e-01, -3.85077606e-01, -5.93091697e-01,\n",
      "        6.36466566e-01,  1.39176484e-02, -3.75905127e-01, -3.23885899e-01,\n",
      "        5.01095821e-01, -2.60832034e-01,  4.37743774e-01,  8.50335558e-01,\n",
      "       -4.40962250e-01,  1.32612463e-01,  6.40362170e-01, -1.81647194e-01,\n",
      "        1.78339976e-01,  1.87521768e+00,  2.54337143e-01, -1.37231380e+00,\n",
      "       -5.55603603e-01, -1.91819573e-01,  2.23660676e+00, -7.04088699e-02,\n",
      "        2.19899312e-01, -3.78011619e-01, -6.29525611e-01,  8.38191832e-01,\n",
      "       -6.08255545e-01,  4.28349455e-01,  6.39833533e-01, -1.27258513e-01,\n",
      "        8.92155272e-01, -1.00219825e-01,  1.02017459e+00, -1.24599523e-01,\n",
      "        1.15358856e+00,  3.68131425e-03,  5.88083795e-01,  1.88818620e+00,\n",
      "        1.12218700e-01,  8.65702454e-01,  1.83446069e+00, -1.31111254e-01,\n",
      "       -3.82292641e-01, -3.22742666e-02, -5.93750604e-02, -2.72684697e-01,\n",
      "        1.41597532e-01,  7.78441426e-01,  2.82149422e-01, -2.96600880e-01,\n",
      "       -7.37766123e-01,  8.82077546e-01, -3.43649492e-01,  1.00499457e+00,\n",
      "        3.58924928e-01, -1.81781582e-01, -1.67772369e-01, -1.06406391e-01,\n",
      "       -1.53537446e-01,  4.85862399e-01, -6.99505868e-01,  7.62390946e-01,\n",
      "        9.42899277e-03,  6.28865988e-01,  3.86020185e-01,  4.11253126e-01,\n",
      "       -4.69211077e-01,  1.25382689e-02, -1.08500475e+00,  1.09834309e+00,\n",
      "       -8.78301147e-01, -3.12972851e-01,  9.84709786e-01, -1.06839574e-01,\n",
      "        8.75131405e-01, -2.55386559e-01,  1.39176484e-02,  1.72060688e+00,\n",
      "        5.19750631e-01,  7.15661688e-01,  2.18330473e+00, -6.31395480e-01,\n",
      "       -4.08529151e-01, -1.60201592e+00,  1.56376771e-01, -1.92555290e-01,\n",
      "       -1.59254433e-01, -1.96414214e-01,  3.86020185e-01, -5.57906941e-01,\n",
      "       -2.64626103e-01,  6.15172703e-01,  9.72191281e-01,  2.19503814e-01,\n",
      "        7.24662455e-01,  7.93553248e-03, -1.49547186e-01, -4.87716970e-01,\n",
      "        1.38443150e-01,  4.13227681e-01, -4.12192821e-01,  4.93580364e-01,\n",
      "       -2.50075360e-01,  2.26139646e-01,  8.11240254e-01,  1.34443983e+00,\n",
      "       -1.04386397e+00, -2.70310676e-01,  2.23587284e-01,  1.30212556e+00,\n",
      "        1.13717338e-01,  3.28083058e-01,  1.44216008e+00, -2.63650635e-01,\n",
      "       -8.05630761e-01,  1.13413675e+00,  4.46746882e-01, -8.97104472e-02,\n",
      "       -1.07461917e+00, -2.31031371e-01,  5.06116349e-02,  2.32168466e-01,\n",
      "        6.65017106e-01, -2.15155437e-01, -3.29975864e-01,  1.35018993e-01,\n",
      "       -8.32848176e-02,  5.01860472e-01, -3.73308346e-01, -3.51651870e-01,\n",
      "       -9.19293618e-01,  2.01598914e-01,  4.41215937e-01,  4.16875298e-01,\n",
      "       -8.00550505e-03,  2.65394537e-01,  1.14691936e+00, -2.26463841e-01,\n",
      "        3.69068212e-01, -5.07406206e-01,  3.10470706e-01,  7.45875560e-01,\n",
      "       -3.07695773e-01,  1.09847110e+00, -8.92402943e-01, -7.62881594e-01,\n",
      "        1.91678755e+00, -1.31142886e-01,  1.33527656e+00,  9.54328724e-01,\n",
      "        1.41252948e+00,  9.56153828e-02,  1.70537539e+00, -2.43767575e-01,\n",
      "       -1.59305284e+00,  6.60740101e-01,  5.36147526e-01, -2.06145765e-01,\n",
      "        2.15111173e+00,  6.75791762e-01,  1.69320471e+00,  1.05671222e+00,\n",
      "       -6.28113427e-01,  1.67839541e+00,  1.42187973e+00,  9.22380397e-01,\n",
      "       -5.55603603e-01,  1.09927679e-01,  6.63856928e-01, -5.14549804e-01,\n",
      "       -6.75457488e-01,  1.23725702e+00,  1.86486390e+00, -3.51625703e-01,\n",
      "        6.35331830e-01,  4.22572373e-01, -2.67843703e-01,  4.30358420e-01,\n",
      "        1.20153902e+00,  1.27551126e+00,  7.30406414e-01, -2.66569782e-03,\n",
      "        9.76406266e-02, -1.47529815e-01,  5.31639220e-01,  1.98613303e+00,\n",
      "       -1.87883923e-01, -1.27341176e-02, -3.49644630e-02,  1.10315503e+00,\n",
      "        1.66923430e-01,  1.08755911e+00, -2.49309078e-01,  1.07117232e-01,\n",
      "       -4.75524283e-01,  8.29691864e-01,  7.83087775e-01, -3.75624675e-01,\n",
      "       -5.00579055e-01,  7.13649150e-01, -4.65099473e-02,  3.83868986e-01,\n",
      "        1.92498677e+00, -1.69315536e-01, -1.99624281e-01,  2.76180305e-01,\n",
      "        8.17458836e-01, -6.15446184e-01,  1.87173185e-01,  4.00736126e-01,\n",
      "        4.86614606e-01,  1.49574690e+00,  1.49972525e-01,  4.92815754e-01,\n",
      "       -3.79242743e-01, -7.77384333e-01,  1.82670011e-01, -1.00165337e+00,\n",
      "        8.64293104e-02,  2.29641818e-02, -3.87444049e-01,  5.93001463e-01,\n",
      "       -3.19757215e-01,  3.80540514e-01,  1.18828482e+00,  5.33692807e-01,\n",
      "       -6.11321575e-01, -5.00364225e-02,  2.43978090e-01, -2.78787040e-01,\n",
      "        4.33697988e-02, -3.10805692e-02, -1.99194390e-02,  7.41404422e-01,\n",
      "        6.07493446e-01, -2.42401318e-01, -4.34630442e-01,  4.26568070e-01,\n",
      "       -1.00707140e+00,  7.05321272e-01, -6.40034260e-01,  5.61360218e-01,\n",
      "       -3.35858765e-01,  2.00594452e-01,  8.24983641e-01, -2.81481398e-01,\n",
      "       -2.86986080e-01,  2.71842782e-01,  7.47745559e-02, -1.61089937e+00,\n",
      "        6.28942829e-01, -6.97397630e-01,  3.67584913e-01, -5.30876420e-01,\n",
      "       -8.07099406e-01,  2.77292935e-01, -2.80338723e-01, -4.69211077e-01,\n",
      "        1.85256160e+00,  5.89489001e-02, -3.79242743e-01, -5.23252886e-01,\n",
      "        7.42575300e-02, -2.03683296e-01,  2.64071495e-01,  5.03030240e-04,\n",
      "        2.60557084e-01,  7.81690281e-02, -4.04704744e-01,  8.66455896e-01,\n",
      "       -1.10154901e-01, -1.42688694e-01,  8.38992752e-01,  2.12932380e-02,\n",
      "        6.46929657e-01, -1.72393555e-01, -7.12678329e-01, -2.57284587e-01,\n",
      "        3.41290212e-01, -4.50761490e-01,  2.46994968e-01,  5.65764049e-02,\n",
      "        2.90570721e-01,  1.63959080e+00,  2.05219490e-01, -2.00577090e-01,\n",
      "        2.53275731e-01,  7.90147984e-02, -4.27974369e-01, -2.75881325e-01,\n",
      "        5.03094997e-04,  3.90008509e-01, -3.22620091e-02,  2.62796110e-01,\n",
      "       -1.29958966e-01,  5.75964819e-01, -5.56586178e-02, -2.75943390e-01,\n",
      "        6.77474322e-01, -1.91913662e-01,  6.80104206e-01, -9.28409900e-01,\n",
      "        2.46827726e-01,  1.73055641e-01, -2.26996424e-01,  2.80471732e-01,\n",
      "        4.05839495e-01,  1.03431023e+00,  4.05891046e-01, -4.29601668e-02,\n",
      "       -4.33889078e-01, -6.92503537e-01,  1.32860497e+00,  1.35163835e+00,\n",
      "        1.64352031e+00,  8.11191956e-01,  6.13700745e-01,  1.84686653e-01,\n",
      "        2.63653797e+00,  1.04053282e-01, -4.63700014e-01,  4.05710675e-01,\n",
      "        2.50098710e+00,  3.14833264e-01,  8.59374220e-01,  9.16688605e-01,\n",
      "        4.26918957e-01, -5.05663604e-01,  2.00018595e-01, -2.71844449e-01,\n",
      "       -5.36581320e-01, -8.17001786e-02,  3.00487530e-01, -2.85239108e-01,\n",
      "        1.61547854e+00,  2.70079751e-01,  2.86602416e-01, -9.61159453e-01,\n",
      "       -1.93056514e-01,  1.20769623e-01,  1.93176198e-01, -5.23252886e-01,\n",
      "        1.39914675e-01, -8.29421430e-02,  5.13405814e-02,  7.33431766e-01,\n",
      "        6.66689562e-01,  5.16484724e-01, -4.60008074e-01,  1.89818100e+00,\n",
      "       -8.27128620e-01,  1.23278596e-01, -9.88139961e-01,  1.84641904e-01,\n",
      "        4.38148352e-01, -3.63951306e-01, -2.40179397e-01, -9.81201074e-02,\n",
      "        1.23541996e-01,  6.05951491e-02,  1.48963750e+00, -7.03433626e-01,\n",
      "        1.21309152e-01,  1.71367056e+00, -1.04805985e-01,  1.52299807e+00,\n",
      "       -3.73899587e-01, -6.64098219e-01,  3.15394084e-01, -2.44451654e-01,\n",
      "        1.62884742e+00,  1.93741915e+00,  6.98000813e-01, -1.76222260e-01,\n",
      "       -2.74507285e-01,  1.90552664e+00, -3.01248606e-01, -1.75096555e+00,\n",
      "        1.26051529e+00, -1.59299909e-01,  1.35043801e-01, -1.04805985e-01,\n",
      "        1.64999587e-02, -5.46845501e-01,  1.60756120e-01,  4.74790552e-01,\n",
      "        6.28811356e-01,  1.87173185e-01,  5.65522284e-01,  1.79462288e+00,\n",
      "       -9.13892281e-02, -3.42620587e-01, -1.22843260e-01,  1.59001808e-01,\n",
      "        9.76406266e-02,  1.83377484e+00, -5.49288074e-02,  1.46403070e-01,\n",
      "       -5.49107931e-01, -6.23364394e-01,  1.40193774e+00, -3.73655634e-01,\n",
      "       -2.89346784e-01,  9.07761614e-01,  1.04574651e+00, -1.15462136e+00,\n",
      "       -1.26063200e+00,  7.84665733e-01, -9.41633629e-01, -1.29502923e-01,\n",
      "       -9.19757578e-01, -1.13669952e-01, -1.75016384e-01,  2.64004822e-01,\n",
      "        2.05001498e+00,  4.29828148e-01, -3.76216164e-01,  1.02680639e+00,\n",
      "        9.00039170e-01, -1.43974360e-01, -3.22742666e-02, -3.00343847e-01]), tensor([0.]), 0, 'NR-AR')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ok",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 91\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave_model!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result , best_model\n\u001b[1;32m---> 91\u001b[0m scores, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(df, k, shuffle)\u001b[0m\n\u001b[0;32m     17\u001b[0m valid_ds \u001b[38;5;241m=\u001b[39m [df[index] \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m test_index]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(valid_ds[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m label_pos , label_neg, _ , _ \u001b[38;5;241m=\u001b[39m count_lablel(train_ds)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain positive label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - train negative label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_neg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: ok"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Epoch_S = 10\n",
    "\n",
    "def evaluate_model(df, k = 10 , shuffle = False):\n",
    "    \n",
    "    result =[] \n",
    "    s = 0\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle= shuffle, random_state=None)\n",
    "    \n",
    "    for train_index, test_index in kf.split(df):\n",
    "\n",
    "        train_ds = [df[index] for index in train_index] \n",
    "        \n",
    "        valid_ds = [df[index] for index in test_index]\n",
    "        \n",
    "        label_pos , label_neg, _ , _ = count_lablel(train_ds)\n",
    "        print(f'train positive label: {label_pos} - train negative label: {label_neg}')\n",
    "        \n",
    "        train_ds = up_and_down_Samplenig(train_ds, scale_downsampling = 0.5)\n",
    "        \n",
    "        label_pos , label_neg , _ , _ = count_lablel(train_ds)\n",
    "        print(f'up and down sampling => train positive label: {label_pos} - train negative label: {label_neg}')\n",
    "\n",
    "        label_pos , label_neg, _ , _ = count_lablel(valid_ds)\n",
    "        print(f'Test positive label: {label_pos} - Test negative label: {label_neg}')\n",
    "\n",
    "        l_train = []\n",
    "        r_train = []\n",
    "        lbls_train = []\n",
    "        l_valid = []\n",
    "        r_valid = []\n",
    "        lbls_valid = []\n",
    "\n",
    "        for i , data in enumerate(train_ds):\n",
    "            embbed_drug, onehot_task, embbed_task, lbl, task_number, task_name = data\n",
    "            l_train.append(embbed_drug[0])\n",
    "            r_train.append(embbed_task)\n",
    "            lbls_train.append(lbl.tolist())\n",
    "        \n",
    "        for i , data in enumerate(valid_ds):\n",
    "            embbed_drug, onehot_task, embbed_task, lbl, task_number, task_name = data\n",
    "            l_valid.append(embbed_drug[0])\n",
    "            r_valid.append(embbed_task)\n",
    "            lbls_valid.append(lbl.tolist())\n",
    "\n",
    "        l_train = np.array(l_train).reshape(-1,512,1)\n",
    "        r_train = np.array(r_train).reshape(-1,512,1)\n",
    "        lbls_train = np.array(lbls_train)\n",
    "\n",
    "        l_valid = np.array(l_valid).reshape(-1,512,1)\n",
    "        r_valid = np.array(r_valid).reshape(-1,512,1)\n",
    "        lbls_valid = np.array(lbls_valid)\n",
    "\n",
    "        # create neural network model\n",
    "        siamese_net = siamese_model_attentiveFp_tox21()\n",
    "        \n",
    "        history = History()\n",
    "        P = siamese_net.fit([l_train, r_train], lbls_train, epochs = Epoch_S, batch_size = 128, callbacks=[history])\n",
    "\n",
    "        for j in range(100):\n",
    "            C=1\n",
    "            Before = int(P.history['accuracy'][-1]*100)\n",
    "            for i in range(2,Epoch_S+1):\n",
    "                if  int(P.history['accuracy'][-i]*100) == Before:\n",
    "                    C=C+1\n",
    "                else:\n",
    "                    C=1\n",
    "                Before=int(P.history['accuracy'][-i]*100)\n",
    "                print(Before)\n",
    "            if C==Epoch_S:\n",
    "                break\n",
    "            P = siamese_net.fit([l_train, r_train], lbls_train, epochs = Epoch_S, batch_size = 128, callbacks=[history])\n",
    "        print(j+1)\n",
    "        \n",
    "        score  = siamese_net.evaluate([l_valid,r_valid], lbls_valid, verbose=1)\n",
    "        a = (score[1],score[4])\n",
    "        result.append(a)\n",
    "        \n",
    "        if score[4] > s :\n",
    "            best_model = siamese_net\n",
    "            s = score[4]\n",
    "            print(\"Save_model!!\")\n",
    "    \n",
    "    return result , best_model\n",
    "\n",
    "\n",
    "scores, best_model = evaluate_model(data_ds, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import latest_checkpoint\n",
    "\n",
    "latest = latest_checkpoint('tox21_model_weights.ckpt')\n",
    "\n",
    "with open(\"scores.pkl\", 'rb') as inp:\n",
    "    scores = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dropout = 0.3 and downsampling = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8864656686782837, 0.894575834274292),\n",
       " (0.8959589600563049, 0.8710899949073792),\n",
       " (0.8937780857086182, 0.8908170461654663),\n",
       " (0.9136626124382019, 0.9047142267227173),\n",
       " (0.9033995866775513, 0.892108678817749),\n",
       " (0.9302116632461548, 0.8991721868515015),\n",
       " (0.9023607969284058, 0.8877696990966797),\n",
       " (0.9074929356575012, 0.8949934840202332),\n",
       " (0.9031305909156799, 0.8929914236068726),\n",
       " (0.9096741080284119, 0.8849363327026367)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.9046135008335113 AUC= 0.8913168907165527 STD_AUC= 0.008552394853305305\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "auc = []\n",
    "for i in scores:\n",
    "    acc.append(i[0])\n",
    "    auc.append(i[1])\n",
    "\n",
    "print(f'accuracy= {np.mean(acc)} AUC= {np.mean(auc)} STD_AUC= {np.std(auc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dropout = 0.2 and downsampling = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9033995866775513, 0.8856201767921448),\n",
       " (0.8992944359779358, 0.8737469911575317),\n",
       " (0.9026299118995667, 0.891227126121521),\n",
       " (0.9049390554428101, 0.8905489444732666),\n",
       " (0.9068633913993835, 0.8824583888053894),\n",
       " (0.9131494760513306, 0.8909887075424194),\n",
       " (0.9098024368286133, 0.8867385387420654),\n",
       " (0.9110854268074036, 0.8700131773948669),\n",
       " (0.9018476009368896, 0.888106644153595),\n",
       " (0.9210931658744812, 0.895555853843689)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.9074104487895965 AUC= 0.885500454902649 STD_AUC= 0.007649230178142343\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "auc = []\n",
    "for i in scores:\n",
    "    acc.append(i[0])\n",
    "    auc.append(i[1])\n",
    "\n",
    "print(f'accuracy= {np.mean(acc)} AUC= {np.mean(auc)} STD_AUC= {np.std(auc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "_dyrk9nGcM81"
   },
   "source": [
    "# Classification with BioAct-Het and Canonical GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "UePCH8kkoBE5",
    "outputId": "9a7ab5d3-9594-44f5-bddb-d47a6c996228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GCN_canonical_Tox21_pre_trained.pth from https://data.dgl.ai/dgllife/pre_trained/gcn_canonical_tox21.pth...\n",
      "Pretrained model loaded\n"
     ]
    }
   ],
   "source": [
    "model_GCN = 'GCN_canonical_Tox21'\n",
    "gcn_model = get_tox21_model(model_GCN)\n",
    "gcn_model.eval()\n",
    "gcn_model = gcn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/7265\n",
      "Processing molecule 2000/7265\n",
      "Processing molecule 3000/7265\n",
      "Processing molecule 4000/7265\n",
      "Processing molecule 5000/7265\n",
      "Processing molecule 6000/7265\n",
      "Processing molecule 7000/7265\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6758\n",
      "Processing molecule 2000/6758\n",
      "Processing molecule 3000/6758\n",
      "Processing molecule 4000/6758\n",
      "Processing molecule 5000/6758\n",
      "Processing molecule 6000/6758\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6549\n",
      "Processing molecule 2000/6549\n",
      "Processing molecule 3000/6549\n",
      "Processing molecule 4000/6549\n",
      "Processing molecule 5000/6549\n",
      "Processing molecule 6000/6549\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5821\n",
      "Processing molecule 2000/5821\n",
      "Processing molecule 3000/5821\n",
      "Processing molecule 4000/5821\n",
      "Processing molecule 5000/5821\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6193\n",
      "Processing molecule 2000/6193\n",
      "Processing molecule 3000/6193\n",
      "Processing molecule 4000/6193\n",
      "Processing molecule 5000/6193\n",
      "Processing molecule 6000/6193\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6955\n",
      "Processing molecule 2000/6955\n",
      "Processing molecule 3000/6955\n",
      "Processing molecule 4000/6955\n",
      "Processing molecule 5000/6955\n",
      "Processing molecule 6000/6955\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6450\n",
      "Processing molecule 2000/6450\n",
      "Processing molecule 3000/6450\n",
      "Processing molecule 4000/6450\n",
      "Processing molecule 5000/6450\n",
      "Processing molecule 6000/6450\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5832\n",
      "Processing molecule 2000/5832\n",
      "Processing molecule 3000/5832\n",
      "Processing molecule 4000/5832\n",
      "Processing molecule 5000/5832\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/7072\n",
      "Processing molecule 2000/7072\n",
      "Processing molecule 3000/7072\n",
      "Processing molecule 4000/7072\n",
      "Processing molecule 5000/7072\n",
      "Processing molecule 6000/7072\n",
      "Processing molecule 7000/7072\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6467\n",
      "Processing molecule 2000/6467\n",
      "Processing molecule 3000/6467\n",
      "Processing molecule 4000/6467\n",
      "Processing molecule 5000/6467\n",
      "Processing molecule 6000/6467\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5810\n",
      "Processing molecule 2000/5810\n",
      "Processing molecule 3000/5810\n",
      "Processing molecule 4000/5810\n",
      "Processing molecule 5000/5810\n",
      "Data created!!\n",
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/6774\n",
      "Processing molecule 2000/6774\n",
      "Processing molecule 3000/6774\n",
      "Processing molecule 4000/6774\n",
      "Processing molecule 5000/6774\n",
      "Processing molecule 6000/6774\n",
      "Data created!!\n"
     ]
    }
   ],
   "source": [
    "data_ds = []\n",
    "for i, task in  enumerate(tox21_tasks):\n",
    "    a = df[['smiles' , task]]\n",
    "    a = a.dropna()\n",
    "    ds =  DATASET(a, smiles_to_bigraph, CanonicalAtomFeaturizer(), cache_file_path = cache_path)\n",
    "    data = create_dataset_with_gcn(ds, embed_class_tox21, gcn_model, tox21_tasks, i)\n",
    "    for d in data:\n",
    "        data_ds.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "hidden": true,
    "id": "rEpMXuj7ndA7",
    "outputId": "7d6726e0-4792-4d06-953e-648851f6117a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive label: 5270 - train negative label: 64881\n",
      "up and down sampling => train positive label: 36890 - train negative label: 64881\n",
      "Test positive label: 592 - Test negative label: 7203\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.5040 - accuracy: 0.7638 - mae: 0.3318 - mse: 0.1649 - auc_10: 0.8151\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4639 - accuracy: 0.7893 - mae: 0.3007 - mse: 0.1491 - auc_10: 0.8459\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4456 - accuracy: 0.7985 - mae: 0.2874 - mse: 0.1426 - auc_10: 0.8590\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4316 - accuracy: 0.8052 - mae: 0.2772 - mse: 0.1377 - auc_10: 0.8688\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4211 - accuracy: 0.8117 - mae: 0.2699 - mse: 0.1340 - auc_10: 0.8756\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4136 - accuracy: 0.8154 - mae: 0.2645 - mse: 0.1315 - auc_10: 0.8804\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4060 - accuracy: 0.8192 - mae: 0.2584 - mse: 0.1287 - auc_10: 0.8854\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3980 - accuracy: 0.8237 - mae: 0.2533 - mse: 0.1260 - auc_10: 0.8900\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3910 - accuracy: 0.8263 - mae: 0.2483 - mse: 0.1236 - auc_10: 0.8945\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3847 - accuracy: 0.8295 - mae: 0.2442 - mse: 0.1216 - auc_10: 0.8980\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "76\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3792 - accuracy: 0.8330 - mae: 0.2403 - mse: 0.1196 - auc_10: 0.9012\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3736 - accuracy: 0.8345 - mae: 0.2369 - mse: 0.1177 - auc_10: 0.9045\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3740 - accuracy: 0.8345 - mae: 0.2367 - mse: 0.1181 - auc_10: 0.9042\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3706 - accuracy: 0.8352 - mae: 0.2350 - mse: 0.1171 - auc_10: 0.9061\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3694 - accuracy: 0.8364 - mae: 0.2341 - mse: 0.1166 - auc_10: 0.9069\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3663 - accuracy: 0.8367 - mae: 0.2327 - mse: 0.1159 - auc_10: 0.9084\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3647 - accuracy: 0.8397 - mae: 0.2307 - mse: 0.1150 - auc_10: 0.9093\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3624 - accuracy: 0.8388 - mae: 0.2293 - mse: 0.1146 - auc_10: 0.9105\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3595 - accuracy: 0.8405 - mae: 0.2278 - mse: 0.1136 - auc_10: 0.9119\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3581 - accuracy: 0.8416 - mae: 0.2265 - mse: 0.1130 - auc_10: 0.9128\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3603 - accuracy: 0.8397 - mae: 0.2287 - mse: 0.1140 - auc_10: 0.9116\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3625 - accuracy: 0.8379 - mae: 0.2301 - mse: 0.1149 - auc_10: 0.9104\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3658 - accuracy: 0.8359 - mae: 0.2330 - mse: 0.1162 - auc_10: 0.9086\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3647 - accuracy: 0.8356 - mae: 0.2323 - mse: 0.1158 - auc_10: 0.9093\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3682 - accuracy: 0.8332 - mae: 0.2348 - mse: 0.1172 - auc_10: 0.9074\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3710 - accuracy: 0.8322 - mae: 0.2370 - mse: 0.1180 - auc_10: 0.9060\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3738 - accuracy: 0.8326 - mae: 0.2381 - mse: 0.1189 - auc_10: 0.9045\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3703 - accuracy: 0.8336 - mae: 0.2362 - mse: 0.1179 - auc_10: 0.9063\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3675 - accuracy: 0.8344 - mae: 0.2342 - mse: 0.1170 - auc_10: 0.9077\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3721 - accuracy: 0.8316 - mae: 0.2374 - mse: 0.1184 - auc_10: 0.9053\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "3\n",
      "244/244 [==============================] - 0s 636us/step - loss: 0.3079 - accuracy: 0.8568 - mae: 0.2120 - mse: 0.0954 - auc_10: 0.8622\n",
      "train positive label: 5296 - train negative label: 64855\n",
      "up and down sampling => train positive label: 37072 - train negative label: 64855\n",
      "Test positive label: 566 - Test negative label: 7229\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.5103 - accuracy: 0.7573 - mae: 0.3374 - mse: 0.1676 - auc_11: 0.8096\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4643 - accuracy: 0.7864 - mae: 0.3015 - mse: 0.1498 - auc_11: 0.8448\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4478 - accuracy: 0.7959 - mae: 0.2891 - mse: 0.1437 - auc_11: 0.8571\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4365 - accuracy: 0.8018 - mae: 0.2808 - mse: 0.1396 - auc_11: 0.8652\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4258 - accuracy: 0.8094 - mae: 0.2727 - mse: 0.1357 - auc_11: 0.8722\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4169 - accuracy: 0.8136 - mae: 0.2661 - mse: 0.1325 - auc_11: 0.8782\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4106 - accuracy: 0.8167 - mae: 0.2618 - mse: 0.1304 - auc_11: 0.8821\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4006 - accuracy: 0.8226 - mae: 0.2548 - mse: 0.1267 - auc_11: 0.8885\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3940 - accuracy: 0.8244 - mae: 0.2505 - mse: 0.1248 - auc_11: 0.8922\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3896 - accuracy: 0.8270 - mae: 0.2471 - mse: 0.1231 - auc_11: 0.8951\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3852 - accuracy: 0.8293 - mae: 0.2441 - mse: 0.1217 - auc_11: 0.8976\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3782 - accuracy: 0.8323 - mae: 0.2397 - mse: 0.1193 - auc_11: 0.9017\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3751 - accuracy: 0.8347 - mae: 0.2371 - mse: 0.1182 - auc_11: 0.9035\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3736 - accuracy: 0.8345 - mae: 0.2365 - mse: 0.1180 - auc_11: 0.9044\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3723 - accuracy: 0.8349 - mae: 0.2358 - mse: 0.1175 - auc_11: 0.9051\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3741 - accuracy: 0.8346 - mae: 0.2371 - mse: 0.1181 - auc_11: 0.9042\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3739 - accuracy: 0.8341 - mae: 0.2368 - mse: 0.1182 - auc_11: 0.9041\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3705 - accuracy: 0.8354 - mae: 0.2346 - mse: 0.1171 - auc_11: 0.9061\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3763 - accuracy: 0.8325 - mae: 0.2388 - mse: 0.1192 - auc_11: 0.9031\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3768 - accuracy: 0.8315 - mae: 0.2394 - mse: 0.1195 - auc_11: 0.9030\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "82\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3755 - accuracy: 0.8320 - mae: 0.2388 - mse: 0.1192 - auc_11: 0.9037\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3737 - accuracy: 0.8339 - mae: 0.2371 - mse: 0.1183 - auc_11: 0.9048\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3718 - accuracy: 0.8337 - mae: 0.2362 - mse: 0.1180 - auc_11: 0.9058\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3759 - accuracy: 0.8312 - mae: 0.2393 - mse: 0.1195 - auc_11: 0.9036\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3791 - accuracy: 0.8289 - mae: 0.2420 - mse: 0.1208 - auc_11: 0.9018\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3780 - accuracy: 0.8290 - mae: 0.2414 - mse: 0.1205 - auc_11: 0.9023\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3743 - accuracy: 0.8320 - mae: 0.2386 - mse: 0.1190 - auc_11: 0.9046\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3702 - accuracy: 0.8336 - mae: 0.2359 - mse: 0.1178 - auc_11: 0.9066\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3744 - accuracy: 0.8308 - mae: 0.2391 - mse: 0.1194 - auc_11: 0.9043\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3832 - accuracy: 0.8254 - mae: 0.2451 - mse: 0.1225 - auc_11: 0.8994\n",
      "83\n",
      "83\n",
      "83\n",
      "82\n",
      "82\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3812 - accuracy: 0.8264 - mae: 0.2444 - mse: 0.1219 - auc_11: 0.9004\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3792 - accuracy: 0.8261 - mae: 0.2428 - mse: 0.1213 - auc_11: 0.9014\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3776 - accuracy: 0.8273 - mae: 0.2424 - mse: 0.1211 - auc_11: 0.9022\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3763 - accuracy: 0.8277 - mae: 0.2416 - mse: 0.1208 - auc_11: 0.9027\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3727 - accuracy: 0.8294 - mae: 0.2391 - mse: 0.1193 - auc_11: 0.9048\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3713 - accuracy: 0.8300 - mae: 0.2377 - mse: 0.1188 - auc_11: 0.9055\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3704 - accuracy: 0.8301 - mae: 0.2375 - mse: 0.1185 - auc_11: 0.9061\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3678 - accuracy: 0.8314 - mae: 0.2356 - mse: 0.1177 - auc_11: 0.9074\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3832 - accuracy: 0.8243 - mae: 0.2458 - mse: 0.1229 - auc_11: 0.8990\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3841 - accuracy: 0.8232 - mae: 0.2463 - mse: 0.1232 - auc_11: 0.8985\n",
      "82\n",
      "83\n",
      "83\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3838 - accuracy: 0.8245 - mae: 0.2468 - mse: 0.1231 - auc_11: 0.8987\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3812 - accuracy: 0.8245 - mae: 0.2450 - mse: 0.1225 - auc_11: 0.9000\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3821 - accuracy: 0.8248 - mae: 0.2455 - mse: 0.1226 - auc_11: 0.8994\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3791 - accuracy: 0.8256 - mae: 0.2437 - mse: 0.1217 - auc_11: 0.9012\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3766 - accuracy: 0.8272 - mae: 0.2418 - mse: 0.1207 - auc_11: 0.9025\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3756 - accuracy: 0.8277 - mae: 0.2415 - mse: 0.1208 - auc_11: 0.9027\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3747 - accuracy: 0.8278 - mae: 0.2410 - mse: 0.1202 - auc_11: 0.9035\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3742 - accuracy: 0.8271 - mae: 0.2409 - mse: 0.1204 - auc_11: 0.9035\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3734 - accuracy: 0.8284 - mae: 0.2395 - mse: 0.1197 - auc_11: 0.9042\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3708 - accuracy: 0.8297 - mae: 0.2380 - mse: 0.1190 - auc_11: 0.9054\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "5\n",
      "244/244 [==============================] - 0s 660us/step - loss: 0.3125 - accuracy: 0.8557 - mae: 0.2101 - mse: 0.0981 - auc_11: 0.8610\n",
      "train positive label: 5267 - train negative label: 64884\n",
      "up and down sampling => train positive label: 36869 - train negative label: 64884\n",
      "Test positive label: 595 - Test negative label: 7200\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.5091 - accuracy: 0.7589 - mae: 0.3358 - mse: 0.1670 - auc_12: 0.8101\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4628 - accuracy: 0.7889 - mae: 0.2998 - mse: 0.1489 - auc_12: 0.8462\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4467 - accuracy: 0.7970 - mae: 0.2881 - mse: 0.1431 - auc_12: 0.8577\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4336 - accuracy: 0.8056 - mae: 0.2783 - mse: 0.1381 - auc_12: 0.8670\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4244 - accuracy: 0.8103 - mae: 0.2713 - mse: 0.1348 - auc_12: 0.8730\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4152 - accuracy: 0.8149 - mae: 0.2646 - mse: 0.1317 - auc_12: 0.8788\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4079 - accuracy: 0.8183 - mae: 0.2594 - mse: 0.1291 - auc_12: 0.8836\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4014 - accuracy: 0.8228 - mae: 0.2546 - mse: 0.1268 - auc_12: 0.8876\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3952 - accuracy: 0.8256 - mae: 0.2503 - mse: 0.1247 - auc_12: 0.8915\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3876 - accuracy: 0.8294 - mae: 0.2454 - mse: 0.1223 - auc_12: 0.8956\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3857 - accuracy: 0.8295 - mae: 0.2442 - mse: 0.1216 - auc_12: 0.8971\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3780 - accuracy: 0.8341 - mae: 0.2390 - mse: 0.1189 - auc_12: 0.9016\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3770 - accuracy: 0.8345 - mae: 0.2380 - mse: 0.1187 - auc_12: 0.9023\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3720 - accuracy: 0.8372 - mae: 0.2347 - mse: 0.1169 - auc_12: 0.9050\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3686 - accuracy: 0.8386 - mae: 0.2328 - mse: 0.1158 - auc_12: 0.9071\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3695 - accuracy: 0.8387 - mae: 0.2327 - mse: 0.1161 - auc_12: 0.9066\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3662 - accuracy: 0.8398 - mae: 0.2308 - mse: 0.1151 - auc_12: 0.9084\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3634 - accuracy: 0.8411 - mae: 0.2294 - mse: 0.1142 - auc_12: 0.9099\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3635 - accuracy: 0.8403 - mae: 0.2294 - mse: 0.1145 - auc_12: 0.9098\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3581 - accuracy: 0.8441 - mae: 0.2256 - mse: 0.1124 - auc_12: 0.9128\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "82\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3593 - accuracy: 0.8417 - mae: 0.2264 - mse: 0.1131 - auc_12: 0.9122\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3545 - accuracy: 0.8451 - mae: 0.2235 - mse: 0.1115 - auc_12: 0.9147\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3560 - accuracy: 0.8428 - mae: 0.2251 - mse: 0.1121 - auc_12: 0.9140\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3631 - accuracy: 0.8386 - mae: 0.2299 - mse: 0.1147 - auc_12: 0.9103\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3664 - accuracy: 0.8375 - mae: 0.2325 - mse: 0.1160 - auc_12: 0.9085\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3696 - accuracy: 0.8360 - mae: 0.2344 - mse: 0.1172 - auc_12: 0.9065\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3682 - accuracy: 0.8358 - mae: 0.2342 - mse: 0.1168 - auc_12: 0.9073\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3645 - accuracy: 0.8378 - mae: 0.2307 - mse: 0.1154 - auc_12: 0.9094\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3637 - accuracy: 0.8376 - mae: 0.2309 - mse: 0.1152 - auc_12: 0.9100\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3630 - accuracy: 0.8382 - mae: 0.2304 - mse: 0.1149 - auc_12: 0.9102\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "84\n",
      "84\n",
      "84\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3643 - accuracy: 0.8368 - mae: 0.2316 - mse: 0.1157 - auc_12: 0.9094\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3642 - accuracy: 0.8354 - mae: 0.2321 - mse: 0.1159 - auc_12: 0.9095\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3686 - accuracy: 0.8341 - mae: 0.2349 - mse: 0.1173 - auc_12: 0.9070\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3674 - accuracy: 0.8340 - mae: 0.2339 - mse: 0.1168 - auc_12: 0.9079\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3622 - accuracy: 0.8367 - mae: 0.2310 - mse: 0.1152 - auc_12: 0.9104\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3620 - accuracy: 0.8378 - mae: 0.2305 - mse: 0.1149 - auc_12: 0.9107\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3623 - accuracy: 0.8368 - mae: 0.2305 - mse: 0.1151 - auc_12: 0.9106\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3582 - accuracy: 0.8380 - mae: 0.2281 - mse: 0.1141 - auc_12: 0.9124\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3575 - accuracy: 0.8396 - mae: 0.2275 - mse: 0.1136 - auc_12: 0.9131\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3614 - accuracy: 0.8371 - mae: 0.2305 - mse: 0.1150 - auc_12: 0.9108\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "4\n",
      "  1/244 [..............................] - ETA: 0s - loss: 0.0928 - accuracy: 1.0000 - mae: 0.0801 - mse: 0.0192 - auc_12: 0.0000e+00WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "244/244 [==============================] - 0s 656us/step - loss: 0.2785 - accuracy: 0.8727 - mae: 0.1843 - mse: 0.0866 - auc_12: 0.8781\n",
      "train positive label: 5306 - train negative label: 64845\n",
      "up and down sampling => train positive label: 37142 - train negative label: 64845\n",
      "Test positive label: 556 - Test negative label: 7239\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.5088 - accuracy: 0.7585 - mae: 0.3366 - mse: 0.1671 - auc_13: 0.8099\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4625 - accuracy: 0.7889 - mae: 0.3001 - mse: 0.1490 - auc_13: 0.8468\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4461 - accuracy: 0.7941 - mae: 0.2880 - mse: 0.1435 - auc_13: 0.8585\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4347 - accuracy: 0.8020 - mae: 0.2795 - mse: 0.1390 - auc_13: 0.8667\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4241 - accuracy: 0.8076 - mae: 0.2714 - mse: 0.1351 - auc_13: 0.8738\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4147 - accuracy: 0.8143 - mae: 0.2644 - mse: 0.1316 - auc_13: 0.8799\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4059 - accuracy: 0.8200 - mae: 0.2578 - mse: 0.1284 - auc_13: 0.8852\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4002 - accuracy: 0.8228 - mae: 0.2536 - mse: 0.1265 - auc_13: 0.8889\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3928 - accuracy: 0.8264 - mae: 0.2492 - mse: 0.1240 - auc_13: 0.8931\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3871 - accuracy: 0.8294 - mae: 0.2451 - mse: 0.1221 - auc_13: 0.8965\n",
      "82\n",
      "82\n",
      "82\n",
      "81\n",
      "80\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3825 - accuracy: 0.8325 - mae: 0.2416 - mse: 0.1204 - auc_13: 0.8993\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3788 - accuracy: 0.8322 - mae: 0.2398 - mse: 0.1195 - auc_13: 0.9014\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3737 - accuracy: 0.8358 - mae: 0.2359 - mse: 0.1176 - auc_13: 0.9043\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3717 - accuracy: 0.8366 - mae: 0.2345 - mse: 0.1169 - auc_13: 0.9055\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3710 - accuracy: 0.8367 - mae: 0.2341 - mse: 0.1167 - auc_13: 0.9059\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3674 - accuracy: 0.8388 - mae: 0.2317 - mse: 0.1155 - auc_13: 0.9079\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3631 - accuracy: 0.8413 - mae: 0.2289 - mse: 0.1140 - auc_13: 0.9102\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3589 - accuracy: 0.8420 - mae: 0.2261 - mse: 0.1128 - auc_13: 0.9124\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3601 - accuracy: 0.8411 - mae: 0.2273 - mse: 0.1134 - auc_13: 0.9118\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3615 - accuracy: 0.8407 - mae: 0.2280 - mse: 0.1137 - auc_13: 0.9113\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3581 - accuracy: 0.8429 - mae: 0.2263 - mse: 0.1128 - auc_13: 0.9129\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3658 - accuracy: 0.8372 - mae: 0.2315 - mse: 0.1155 - auc_13: 0.9088\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3632 - accuracy: 0.8398 - mae: 0.2291 - mse: 0.1144 - auc_13: 0.9101\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3549 - accuracy: 0.8434 - mae: 0.2240 - mse: 0.1118 - auc_13: 0.9146\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3524 - accuracy: 0.8450 - mae: 0.2221 - mse: 0.1109 - auc_13: 0.9158\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3541 - accuracy: 0.8430 - mae: 0.2242 - mse: 0.1118 - auc_13: 0.9151\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3527 - accuracy: 0.8431 - mae: 0.2226 - mse: 0.1112 - auc_13: 0.9159\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3498 - accuracy: 0.8456 - mae: 0.2210 - mse: 0.1103 - auc_13: 0.9172\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3475 - accuracy: 0.8475 - mae: 0.2192 - mse: 0.1094 - auc_13: 0.9183\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3478 - accuracy: 0.8464 - mae: 0.2195 - mse: 0.1094 - auc_13: 0.9183\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "84\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3542 - accuracy: 0.8432 - mae: 0.2237 - mse: 0.1116 - auc_13: 0.9150\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3552 - accuracy: 0.8418 - mae: 0.2247 - mse: 0.1123 - auc_13: 0.9142\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3679 - accuracy: 0.8371 - mae: 0.2331 - mse: 0.1165 - auc_13: 0.9079\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3726 - accuracy: 0.8344 - mae: 0.2366 - mse: 0.1182 - auc_13: 0.9053\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3833 - accuracy: 0.8272 - mae: 0.2449 - mse: 0.1221 - auc_13: 0.8993\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3836 - accuracy: 0.8276 - mae: 0.2448 - mse: 0.1223 - auc_13: 0.8988\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3823 - accuracy: 0.8294 - mae: 0.2435 - mse: 0.1216 - auc_13: 0.8999\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3794 - accuracy: 0.8307 - mae: 0.2418 - mse: 0.1208 - auc_13: 0.9015\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3783 - accuracy: 0.8301 - mae: 0.2419 - mse: 0.1205 - auc_13: 0.9020\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3791 - accuracy: 0.8293 - mae: 0.2416 - mse: 0.1209 - auc_13: 0.9015\n",
      "83\n",
      "83\n",
      "82\n",
      "82\n",
      "82\n",
      "83\n",
      "83\n",
      "84\n",
      "84\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3805 - accuracy: 0.8282 - mae: 0.2430 - mse: 0.1214 - auc_13: 0.9007\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3811 - accuracy: 0.8281 - mae: 0.2436 - mse: 0.1217 - auc_13: 0.9005\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3804 - accuracy: 0.8268 - mae: 0.2437 - mse: 0.1218 - auc_13: 0.9008\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3797 - accuracy: 0.8273 - mae: 0.2435 - mse: 0.1216 - auc_13: 0.9011\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3775 - accuracy: 0.8283 - mae: 0.2420 - mse: 0.1208 - auc_13: 0.9021\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3780 - accuracy: 0.8279 - mae: 0.2421 - mse: 0.1210 - auc_13: 0.9019\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3770 - accuracy: 0.8282 - mae: 0.2421 - mse: 0.1208 - auc_13: 0.9025\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3756 - accuracy: 0.8287 - mae: 0.2409 - mse: 0.1204 - auc_13: 0.9032\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3733 - accuracy: 0.8299 - mae: 0.2397 - mse: 0.1197 - auc_13: 0.9044\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3755 - accuracy: 0.8290 - mae: 0.2406 - mse: 0.1202 - auc_13: 0.9034\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "5\n",
      "244/244 [==============================] - 0s 640us/step - loss: 0.3047 - accuracy: 0.8552 - mae: 0.2077 - mse: 0.0942 - auc_13: 0.8481\n",
      "train positive label: 5245 - train negative label: 64906\n",
      "up and down sampling => train positive label: 36715 - train negative label: 64906\n",
      "Test positive label: 617 - Test negative label: 7178\n",
      "Epoch 1/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.5107 - accuracy: 0.7557 - mae: 0.3376 - mse: 0.1676 - auc_14: 0.8096\n",
      "Epoch 2/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.4597 - accuracy: 0.7881 - mae: 0.2985 - mse: 0.1481 - auc_14: 0.8488\n",
      "Epoch 3/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.4404 - accuracy: 0.8003 - mae: 0.2838 - mse: 0.1410 - auc_14: 0.8619\n",
      "Epoch 4/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.4275 - accuracy: 0.8076 - mae: 0.2744 - mse: 0.1361 - auc_14: 0.8711\n",
      "Epoch 5/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.4185 - accuracy: 0.8119 - mae: 0.2679 - mse: 0.1332 - auc_14: 0.8768\n",
      "Epoch 6/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.4093 - accuracy: 0.8177 - mae: 0.2613 - mse: 0.1298 - auc_14: 0.8826\n",
      "Epoch 7/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.4004 - accuracy: 0.8219 - mae: 0.2552 - mse: 0.1268 - auc_14: 0.8884\n",
      "Epoch 8/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3935 - accuracy: 0.8247 - mae: 0.2504 - mse: 0.1246 - auc_14: 0.8923\n",
      "Epoch 9/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3874 - accuracy: 0.8272 - mae: 0.2463 - mse: 0.1224 - auc_14: 0.8962\n",
      "Epoch 10/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3817 - accuracy: 0.8303 - mae: 0.2424 - mse: 0.1207 - auc_14: 0.8993\n",
      "82\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3771 - accuracy: 0.8327 - mae: 0.2392 - mse: 0.1189 - auc_14: 0.9021\n",
      "Epoch 2/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3730 - accuracy: 0.8356 - mae: 0.2362 - mse: 0.1176 - auc_14: 0.9044\n",
      "Epoch 3/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3699 - accuracy: 0.8364 - mae: 0.2341 - mse: 0.1166 - auc_14: 0.9063\n",
      "Epoch 4/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3630 - accuracy: 0.8399 - mae: 0.2295 - mse: 0.1143 - auc_14: 0.9100\n",
      "Epoch 5/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3614 - accuracy: 0.8400 - mae: 0.2286 - mse: 0.1139 - auc_14: 0.9107\n",
      "Epoch 6/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3562 - accuracy: 0.8424 - mae: 0.2254 - mse: 0.1122 - auc_14: 0.9136\n",
      "Epoch 7/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3563 - accuracy: 0.8413 - mae: 0.2256 - mse: 0.1125 - auc_14: 0.9135\n",
      "Epoch 8/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3538 - accuracy: 0.8442 - mae: 0.2234 - mse: 0.1113 - auc_14: 0.9148\n",
      "Epoch 9/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3509 - accuracy: 0.8463 - mae: 0.2212 - mse: 0.1101 - auc_14: 0.9163\n",
      "Epoch 10/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3503 - accuracy: 0.8454 - mae: 0.2208 - mse: 0.1101 - auc_14: 0.9167\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3557 - accuracy: 0.8444 - mae: 0.2243 - mse: 0.1119 - auc_14: 0.9138\n",
      "Epoch 2/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3591 - accuracy: 0.8422 - mae: 0.2269 - mse: 0.1133 - auc_14: 0.9119\n",
      "Epoch 3/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3580 - accuracy: 0.8434 - mae: 0.2264 - mse: 0.1129 - auc_14: 0.9126\n",
      "Epoch 4/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3567 - accuracy: 0.8438 - mae: 0.2252 - mse: 0.1124 - auc_14: 0.9133\n",
      "Epoch 5/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3516 - accuracy: 0.8457 - mae: 0.2219 - mse: 0.1108 - auc_14: 0.9159\n",
      "Epoch 6/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3508 - accuracy: 0.8455 - mae: 0.2217 - mse: 0.1106 - auc_14: 0.9163\n",
      "Epoch 7/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3527 - accuracy: 0.8458 - mae: 0.2229 - mse: 0.1113 - auc_14: 0.9153\n",
      "Epoch 8/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3507 - accuracy: 0.8455 - mae: 0.2218 - mse: 0.1108 - auc_14: 0.9162\n",
      "Epoch 9/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3528 - accuracy: 0.8448 - mae: 0.2231 - mse: 0.1113 - auc_14: 0.9153\n",
      "Epoch 10/10\n",
      "794/794 [==============================] - 3s 4ms/step - loss: 0.3599 - accuracy: 0.8403 - mae: 0.2287 - mse: 0.1141 - auc_14: 0.9116\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "3\n",
      "244/244 [==============================] - 0s 636us/step - loss: 0.3090 - accuracy: 0.8622 - mae: 0.2056 - mse: 0.0965 - auc_14: 0.8729\n",
      "train positive label: 5272 - train negative label: 64879\n",
      "up and down sampling => train positive label: 36904 - train negative label: 64879\n",
      "Test positive label: 590 - Test negative label: 7205\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.5101 - accuracy: 0.7578 - mae: 0.3362 - mse: 0.1674 - auc_15: 0.8092\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4654 - accuracy: 0.7851 - mae: 0.3023 - mse: 0.1501 - auc_15: 0.8442\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4448 - accuracy: 0.7961 - mae: 0.2872 - mse: 0.1426 - auc_15: 0.8591\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4333 - accuracy: 0.8040 - mae: 0.2786 - mse: 0.1384 - auc_15: 0.8673\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4227 - accuracy: 0.8082 - mae: 0.2708 - mse: 0.1347 - auc_15: 0.8744\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4130 - accuracy: 0.8142 - mae: 0.2637 - mse: 0.1312 - auc_15: 0.8808\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4073 - accuracy: 0.8181 - mae: 0.2597 - mse: 0.1291 - auc_15: 0.8841\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3980 - accuracy: 0.8227 - mae: 0.2532 - mse: 0.1261 - auc_15: 0.8898\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3945 - accuracy: 0.8251 - mae: 0.2502 - mse: 0.1246 - auc_15: 0.8920\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3879 - accuracy: 0.8288 - mae: 0.2460 - mse: 0.1225 - auc_15: 0.8959\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3831 - accuracy: 0.8311 - mae: 0.2426 - mse: 0.1207 - auc_15: 0.8986\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3778 - accuracy: 0.8330 - mae: 0.2394 - mse: 0.1193 - auc_15: 0.9018\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3750 - accuracy: 0.8346 - mae: 0.2372 - mse: 0.1182 - auc_15: 0.9032\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3725 - accuracy: 0.8357 - mae: 0.2356 - mse: 0.1174 - auc_15: 0.9048\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3696 - accuracy: 0.8366 - mae: 0.2339 - mse: 0.1165 - auc_15: 0.9064\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3644 - accuracy: 0.8391 - mae: 0.2303 - mse: 0.1149 - auc_15: 0.9093\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3612 - accuracy: 0.8400 - mae: 0.2287 - mse: 0.1139 - auc_15: 0.9111\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3646 - accuracy: 0.8394 - mae: 0.2299 - mse: 0.1148 - auc_15: 0.9093\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3597 - accuracy: 0.8423 - mae: 0.2270 - mse: 0.1130 - auc_15: 0.9120\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3605 - accuracy: 0.8412 - mae: 0.2277 - mse: 0.1136 - auc_15: 0.9116\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3626 - accuracy: 0.8403 - mae: 0.2289 - mse: 0.1144 - auc_15: 0.9105\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3607 - accuracy: 0.8389 - mae: 0.2288 - mse: 0.1140 - auc_15: 0.9115\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3602 - accuracy: 0.8393 - mae: 0.2281 - mse: 0.1140 - auc_15: 0.9117\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3636 - accuracy: 0.8382 - mae: 0.2305 - mse: 0.1149 - auc_15: 0.9100\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3593 - accuracy: 0.8416 - mae: 0.2270 - mse: 0.1133 - auc_15: 0.9123\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3584 - accuracy: 0.8419 - mae: 0.2269 - mse: 0.1130 - auc_15: 0.9128\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3630 - accuracy: 0.8380 - mae: 0.2302 - mse: 0.1150 - auc_15: 0.9102\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3623 - accuracy: 0.8385 - mae: 0.2299 - mse: 0.1148 - auc_15: 0.9107\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3695 - accuracy: 0.8351 - mae: 0.2339 - mse: 0.1169 - auc_15: 0.9068\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3699 - accuracy: 0.8354 - mae: 0.2348 - mse: 0.1171 - auc_15: 0.9065\n",
      "83\n",
      "83\n",
      "83\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "84\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3691 - accuracy: 0.8354 - mae: 0.2343 - mse: 0.1170 - auc_15: 0.9069\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3685 - accuracy: 0.8363 - mae: 0.2338 - mse: 0.1167 - auc_15: 0.9073\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3692 - accuracy: 0.8340 - mae: 0.2345 - mse: 0.1172 - auc_15: 0.9068\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3673 - accuracy: 0.8355 - mae: 0.2334 - mse: 0.1166 - auc_15: 0.9078\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3659 - accuracy: 0.8366 - mae: 0.2321 - mse: 0.1159 - auc_15: 0.9086\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3646 - accuracy: 0.8374 - mae: 0.2315 - mse: 0.1156 - auc_15: 0.9094\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3615 - accuracy: 0.8391 - mae: 0.2292 - mse: 0.1145 - auc_15: 0.9109\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3599 - accuracy: 0.8382 - mae: 0.2288 - mse: 0.1142 - auc_15: 0.9117\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3603 - accuracy: 0.8390 - mae: 0.2286 - mse: 0.1142 - auc_15: 0.9117\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3583 - accuracy: 0.8387 - mae: 0.2276 - mse: 0.1135 - auc_15: 0.9127\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "4\n",
      "244/244 [==============================] - 0s 648us/step - loss: 0.2773 - accuracy: 0.8745 - mae: 0.1803 - mse: 0.0859 - auc_15: 0.8670\n",
      "train positive label: 5302 - train negative label: 64850\n",
      "up and down sampling => train positive label: 37114 - train negative label: 64850\n",
      "Test positive label: 560 - Test negative label: 7234\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.5077 - accuracy: 0.7589 - mae: 0.3341 - mse: 0.1666 - auc_16: 0.8102\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4650 - accuracy: 0.7855 - mae: 0.3017 - mse: 0.1501 - auc_16: 0.8445\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4498 - accuracy: 0.7954 - mae: 0.2900 - mse: 0.1443 - auc_16: 0.8558\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4384 - accuracy: 0.8017 - mae: 0.2820 - mse: 0.1402 - auc_16: 0.8637\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4292 - accuracy: 0.8075 - mae: 0.2748 - mse: 0.1369 - auc_16: 0.8701\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4197 - accuracy: 0.8121 - mae: 0.2683 - mse: 0.1334 - auc_16: 0.8766\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4104 - accuracy: 0.8168 - mae: 0.2619 - mse: 0.1304 - auc_16: 0.8824\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.4049 - accuracy: 0.8199 - mae: 0.2577 - mse: 0.1282 - auc_16: 0.8860\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3977 - accuracy: 0.8249 - mae: 0.2524 - mse: 0.1257 - auc_16: 0.8902\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3931 - accuracy: 0.8269 - mae: 0.2494 - mse: 0.1243 - auc_16: 0.8931\n",
      "82\n",
      "81\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3864 - accuracy: 0.8300 - mae: 0.2454 - mse: 0.1220 - auc_16: 0.8969\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3821 - accuracy: 0.8316 - mae: 0.2421 - mse: 0.1208 - auc_16: 0.8994\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3747 - accuracy: 0.8358 - mae: 0.2369 - mse: 0.1179 - auc_16: 0.9039\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3726 - accuracy: 0.8354 - mae: 0.2356 - mse: 0.1175 - auc_16: 0.9050\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3671 - accuracy: 0.8373 - mae: 0.2324 - mse: 0.1157 - auc_16: 0.9081\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3645 - accuracy: 0.8379 - mae: 0.2309 - mse: 0.1151 - auc_16: 0.9095\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3603 - accuracy: 0.8427 - mae: 0.2278 - mse: 0.1135 - auc_16: 0.9115\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3574 - accuracy: 0.8423 - mae: 0.2259 - mse: 0.1126 - auc_16: 0.9133\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3571 - accuracy: 0.8430 - mae: 0.2253 - mse: 0.1123 - auc_16: 0.9135\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3515 - accuracy: 0.8460 - mae: 0.2219 - mse: 0.1105 - auc_16: 0.9163\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3528 - accuracy: 0.8446 - mae: 0.2227 - mse: 0.1111 - auc_16: 0.9157\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3514 - accuracy: 0.8452 - mae: 0.2220 - mse: 0.1107 - auc_16: 0.9165\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3506 - accuracy: 0.8458 - mae: 0.2215 - mse: 0.1105 - auc_16: 0.9168\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3534 - accuracy: 0.8433 - mae: 0.2237 - mse: 0.1116 - auc_16: 0.9156\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3559 - accuracy: 0.8411 - mae: 0.2262 - mse: 0.1127 - auc_16: 0.9142\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3551 - accuracy: 0.8417 - mae: 0.2252 - mse: 0.1123 - auc_16: 0.9147\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3528 - accuracy: 0.8434 - mae: 0.2233 - mse: 0.1114 - auc_16: 0.9158\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3541 - accuracy: 0.8436 - mae: 0.2242 - mse: 0.1119 - auc_16: 0.9151\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3541 - accuracy: 0.8424 - mae: 0.2247 - mse: 0.1121 - auc_16: 0.9151\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 3s 4ms/step - loss: 0.3562 - accuracy: 0.8412 - mae: 0.2261 - mse: 0.1128 - auc_16: 0.9140\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "3\n",
      "244/244 [==============================] - 0s 632us/step - loss: 0.2692 - accuracy: 0.8867 - mae: 0.1827 - mse: 0.0815 - auc_16: 0.8893\n",
      "train positive label: 5258 - train negative label: 64894\n",
      "up and down sampling => train positive label: 36806 - train negative label: 64894\n",
      "Test positive label: 604 - Test negative label: 7190\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.5062 - accuracy: 0.7604 - mae: 0.3350 - mse: 0.1660 - auc_17: 0.8128\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4581 - accuracy: 0.7886 - mae: 0.2975 - mse: 0.1476 - auc_17: 0.8501\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4397 - accuracy: 0.8004 - mae: 0.2833 - mse: 0.1407 - auc_17: 0.8630\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4277 - accuracy: 0.8066 - mae: 0.2746 - mse: 0.1364 - auc_17: 0.8709\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4176 - accuracy: 0.8122 - mae: 0.2669 - mse: 0.1329 - auc_17: 0.8776\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4104 - accuracy: 0.8181 - mae: 0.2619 - mse: 0.1302 - auc_17: 0.8820\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.4028 - accuracy: 0.8209 - mae: 0.2564 - mse: 0.1276 - auc_17: 0.8871\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3958 - accuracy: 0.8252 - mae: 0.2513 - mse: 0.1252 - auc_17: 0.8912\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3896 - accuracy: 0.8277 - mae: 0.2469 - mse: 0.1231 - auc_17: 0.8950\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3857 - accuracy: 0.8304 - mae: 0.2444 - mse: 0.1216 - auc_17: 0.8973\n",
      "82\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "78\n",
      "76\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3804 - accuracy: 0.8324 - mae: 0.2406 - mse: 0.1198 - auc_17: 0.9006\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3752 - accuracy: 0.8354 - mae: 0.2371 - mse: 0.1180 - auc_17: 0.9032\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3696 - accuracy: 0.8386 - mae: 0.2331 - mse: 0.1161 - auc_17: 0.9065\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3667 - accuracy: 0.8397 - mae: 0.2311 - mse: 0.1153 - auc_17: 0.9079\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3637 - accuracy: 0.8408 - mae: 0.2295 - mse: 0.1144 - auc_17: 0.9096\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3634 - accuracy: 0.8399 - mae: 0.2296 - mse: 0.1145 - auc_17: 0.9100\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3602 - accuracy: 0.8424 - mae: 0.2275 - mse: 0.1133 - auc_17: 0.9115\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3560 - accuracy: 0.8438 - mae: 0.2245 - mse: 0.1119 - auc_17: 0.9137\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3513 - accuracy: 0.8454 - mae: 0.2214 - mse: 0.1106 - auc_17: 0.9161\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3499 - accuracy: 0.8471 - mae: 0.2209 - mse: 0.1100 - auc_17: 0.9168\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3511 - accuracy: 0.8454 - mae: 0.2218 - mse: 0.1108 - auc_17: 0.9163\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3517 - accuracy: 0.8457 - mae: 0.2224 - mse: 0.1108 - auc_17: 0.9159\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3570 - accuracy: 0.8420 - mae: 0.2261 - mse: 0.1129 - auc_17: 0.9131\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3539 - accuracy: 0.8434 - mae: 0.2239 - mse: 0.1117 - auc_17: 0.9148\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3527 - accuracy: 0.8436 - mae: 0.2229 - mse: 0.1113 - auc_17: 0.9155\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3511 - accuracy: 0.8447 - mae: 0.2220 - mse: 0.1108 - auc_17: 0.9163\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3536 - accuracy: 0.8430 - mae: 0.2241 - mse: 0.1118 - auc_17: 0.9149\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3540 - accuracy: 0.8422 - mae: 0.2244 - mse: 0.1121 - auc_17: 0.9148\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3610 - accuracy: 0.8373 - mae: 0.2300 - mse: 0.1148 - auc_17: 0.9111\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3677 - accuracy: 0.8334 - mae: 0.2349 - mse: 0.1172 - auc_17: 0.9076\n",
      "83\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "Epoch 1/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3640 - accuracy: 0.8350 - mae: 0.2323 - mse: 0.1161 - auc_17: 0.9095\n",
      "Epoch 2/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3665 - accuracy: 0.8324 - mae: 0.2346 - mse: 0.1171 - auc_17: 0.9082\n",
      "Epoch 3/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3628 - accuracy: 0.8346 - mae: 0.2321 - mse: 0.1157 - auc_17: 0.9102\n",
      "Epoch 4/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3624 - accuracy: 0.8360 - mae: 0.2311 - mse: 0.1156 - auc_17: 0.9103\n",
      "Epoch 5/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3624 - accuracy: 0.8353 - mae: 0.2316 - mse: 0.1156 - auc_17: 0.9103\n",
      "Epoch 6/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3562 - accuracy: 0.8379 - mae: 0.2278 - mse: 0.1138 - auc_17: 0.9134\n",
      "Epoch 7/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3541 - accuracy: 0.8385 - mae: 0.2264 - mse: 0.1128 - auc_17: 0.9146\n",
      "Epoch 8/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3555 - accuracy: 0.8384 - mae: 0.2270 - mse: 0.1134 - auc_17: 0.9139\n",
      "Epoch 9/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3549 - accuracy: 0.8388 - mae: 0.2269 - mse: 0.1132 - auc_17: 0.9142\n",
      "Epoch 10/10\n",
      "795/795 [==============================] - 3s 4ms/step - loss: 0.3592 - accuracy: 0.8366 - mae: 0.2299 - mse: 0.1149 - auc_17: 0.9118\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "4\n",
      "244/244 [==============================] - 0s 640us/step - loss: 0.2871 - accuracy: 0.8628 - mae: 0.1912 - mse: 0.0889 - auc_17: 0.8677\n",
      "train positive label: 5270 - train negative label: 64882\n",
      "up and down sampling => train positive label: 36890 - train negative label: 64882\n",
      "Test positive label: 592 - Test negative label: 7202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.5079 - accuracy: 0.7584 - mae: 0.3347 - mse: 0.1661 - auc_18: 0.8123\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4638 - accuracy: 0.7866 - mae: 0.3010 - mse: 0.1494 - auc_18: 0.8459\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4468 - accuracy: 0.7954 - mae: 0.2887 - mse: 0.1433 - auc_18: 0.8582\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4337 - accuracy: 0.8030 - mae: 0.2789 - mse: 0.1386 - auc_18: 0.8668\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4236 - accuracy: 0.8093 - mae: 0.2709 - mse: 0.1349 - auc_18: 0.8734\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4156 - accuracy: 0.8140 - mae: 0.2657 - mse: 0.1321 - auc_18: 0.8786\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4059 - accuracy: 0.8189 - mae: 0.2583 - mse: 0.1285 - auc_18: 0.8849\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4005 - accuracy: 0.8213 - mae: 0.2542 - mse: 0.1266 - auc_18: 0.8884\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3951 - accuracy: 0.8259 - mae: 0.2504 - mse: 0.1246 - auc_18: 0.8915\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3896 - accuracy: 0.8288 - mae: 0.2464 - mse: 0.1226 - auc_18: 0.8950\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3871 - accuracy: 0.8293 - mae: 0.2452 - mse: 0.1220 - auc_18: 0.8965\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3813 - accuracy: 0.8320 - mae: 0.2409 - mse: 0.1201 - auc_18: 0.8996\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3781 - accuracy: 0.8332 - mae: 0.2386 - mse: 0.1190 - auc_18: 0.9016\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3786 - accuracy: 0.8333 - mae: 0.2392 - mse: 0.1193 - auc_18: 0.9012\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3770 - accuracy: 0.8323 - mae: 0.2380 - mse: 0.1186 - auc_18: 0.9022\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3738 - accuracy: 0.8344 - mae: 0.2362 - mse: 0.1177 - auc_18: 0.9041\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3713 - accuracy: 0.8361 - mae: 0.2343 - mse: 0.1170 - auc_18: 0.9055\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3703 - accuracy: 0.8366 - mae: 0.2337 - mse: 0.1166 - auc_18: 0.9061\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3653 - accuracy: 0.8399 - mae: 0.2298 - mse: 0.1148 - auc_18: 0.9086\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3615 - accuracy: 0.8400 - mae: 0.2276 - mse: 0.1137 - auc_18: 0.9108\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "82\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3597 - accuracy: 0.8419 - mae: 0.2263 - mse: 0.1129 - auc_18: 0.9118\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3602 - accuracy: 0.8417 - mae: 0.2268 - mse: 0.1131 - auc_18: 0.9115\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3581 - accuracy: 0.8420 - mae: 0.2253 - mse: 0.1124 - auc_18: 0.9127\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3545 - accuracy: 0.8438 - mae: 0.2229 - mse: 0.1113 - auc_18: 0.9145\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3544 - accuracy: 0.8451 - mae: 0.2228 - mse: 0.1112 - auc_18: 0.9145\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3533 - accuracy: 0.8441 - mae: 0.2220 - mse: 0.1110 - auc_18: 0.9151\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3512 - accuracy: 0.8451 - mae: 0.2212 - mse: 0.1104 - auc_18: 0.9163\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3536 - accuracy: 0.8435 - mae: 0.2228 - mse: 0.1113 - auc_18: 0.9149\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3632 - accuracy: 0.8398 - mae: 0.2285 - mse: 0.1143 - auc_18: 0.9100\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3705 - accuracy: 0.8359 - mae: 0.2338 - mse: 0.1169 - auc_18: 0.9060\n",
      "83\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3685 - accuracy: 0.8356 - mae: 0.2332 - mse: 0.1164 - auc_18: 0.9072\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3682 - accuracy: 0.8371 - mae: 0.2329 - mse: 0.1163 - auc_18: 0.9074\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3684 - accuracy: 0.8364 - mae: 0.2332 - mse: 0.1164 - auc_18: 0.9073\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3815 - accuracy: 0.8296 - mae: 0.2421 - mse: 0.1210 - auc_18: 0.9002\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3790 - accuracy: 0.8284 - mae: 0.2414 - mse: 0.1204 - auc_18: 0.9016\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3776 - accuracy: 0.8296 - mae: 0.2404 - mse: 0.1201 - auc_18: 0.9021\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3761 - accuracy: 0.8299 - mae: 0.2393 - mse: 0.1194 - auc_18: 0.9032\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3869 - accuracy: 0.8237 - mae: 0.2470 - mse: 0.1234 - auc_18: 0.8971\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3901 - accuracy: 0.8218 - mae: 0.2499 - mse: 0.1248 - auc_18: 0.8952\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3970 - accuracy: 0.8172 - mae: 0.2550 - mse: 0.1274 - auc_18: 0.8911\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "82\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3971 - accuracy: 0.8172 - mae: 0.2546 - mse: 0.1273 - auc_18: 0.8910\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3982 - accuracy: 0.8159 - mae: 0.2560 - mse: 0.1278 - auc_18: 0.8903\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4112 - accuracy: 0.8083 - mae: 0.2660 - mse: 0.1328 - auc_18: 0.8823\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4106 - accuracy: 0.8087 - mae: 0.2657 - mse: 0.1328 - auc_18: 0.8826\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4088 - accuracy: 0.8087 - mae: 0.2645 - mse: 0.1322 - auc_18: 0.8834\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4075 - accuracy: 0.8093 - mae: 0.2640 - mse: 0.1318 - auc_18: 0.8843\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4062 - accuracy: 0.8090 - mae: 0.2633 - mse: 0.1314 - auc_18: 0.8852\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4280 - accuracy: 0.7973 - mae: 0.2782 - mse: 0.1390 - auc_18: 0.8714\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4274 - accuracy: 0.7961 - mae: 0.2781 - mse: 0.1391 - auc_18: 0.8716\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4264 - accuracy: 0.7986 - mae: 0.2776 - mse: 0.1387 - auc_18: 0.8722\n",
      "79\n",
      "79\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "81\n",
      "81\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4256 - accuracy: 0.8004 - mae: 0.2765 - mse: 0.1382 - auc_18: 0.8728\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4232 - accuracy: 0.7989 - mae: 0.2755 - mse: 0.1376 - auc_18: 0.8742\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4245 - accuracy: 0.7992 - mae: 0.2761 - mse: 0.1381 - auc_18: 0.8734\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4204 - accuracy: 0.7999 - mae: 0.2743 - mse: 0.1371 - auc_18: 0.8756\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4207 - accuracy: 0.7997 - mae: 0.2741 - mse: 0.1371 - auc_18: 0.8757\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4200 - accuracy: 0.8008 - mae: 0.2737 - mse: 0.1367 - auc_18: 0.8761\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4186 - accuracy: 0.8011 - mae: 0.2727 - mse: 0.1363 - auc_18: 0.8769\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4177 - accuracy: 0.8016 - mae: 0.2720 - mse: 0.1359 - auc_18: 0.8775\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4174 - accuracy: 0.8016 - mae: 0.2722 - mse: 0.1359 - auc_18: 0.8776\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4154 - accuracy: 0.8035 - mae: 0.2706 - mse: 0.1352 - auc_18: 0.8788\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "79\n",
      "79\n",
      "79\n",
      "79\n",
      "80\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4136 - accuracy: 0.8037 - mae: 0.2694 - mse: 0.1346 - auc_18: 0.8799\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4139 - accuracy: 0.8027 - mae: 0.2703 - mse: 0.1350 - auc_18: 0.8796\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4140 - accuracy: 0.8036 - mae: 0.2696 - mse: 0.1348 - auc_18: 0.8797\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4122 - accuracy: 0.8045 - mae: 0.2685 - mse: 0.1342 - auc_18: 0.8807\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4118 - accuracy: 0.8043 - mae: 0.2687 - mse: 0.1342 - auc_18: 0.8810\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4110 - accuracy: 0.8044 - mae: 0.2685 - mse: 0.1341 - auc_18: 0.8812\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4111 - accuracy: 0.8042 - mae: 0.2679 - mse: 0.1341 - auc_18: 0.8812\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4093 - accuracy: 0.8058 - mae: 0.2673 - mse: 0.1334 - auc_18: 0.8823\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4088 - accuracy: 0.8052 - mae: 0.2668 - mse: 0.1334 - auc_18: 0.8825\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4096 - accuracy: 0.8054 - mae: 0.2675 - mse: 0.1335 - auc_18: 0.8820\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "7\n",
      "244/244 [==============================] - 0s 640us/step - loss: 0.3440 - accuracy: 0.8418 - mae: 0.2264 - mse: 0.1084 - auc_18: 0.8096\n",
      "train positive label: 5272 - train negative label: 64880\n",
      "up and down sampling => train positive label: 36904 - train negative label: 64880\n",
      "Test positive label: 590 - Test negative label: 7204\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.5096 - accuracy: 0.7586 - mae: 0.3367 - mse: 0.1673 - auc_19: 0.8096\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4662 - accuracy: 0.7884 - mae: 0.3026 - mse: 0.1502 - auc_19: 0.8433\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4485 - accuracy: 0.7958 - mae: 0.2898 - mse: 0.1440 - auc_19: 0.8560\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4369 - accuracy: 0.8041 - mae: 0.2809 - mse: 0.1396 - auc_19: 0.8643\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4263 - accuracy: 0.8097 - mae: 0.2731 - mse: 0.1358 - auc_19: 0.8715\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4182 - accuracy: 0.8144 - mae: 0.2671 - mse: 0.1327 - auc_19: 0.8771\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4092 - accuracy: 0.8181 - mae: 0.2607 - mse: 0.1298 - auc_19: 0.8828\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4010 - accuracy: 0.8232 - mae: 0.2549 - mse: 0.1268 - auc_19: 0.8878\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3946 - accuracy: 0.8259 - mae: 0.2502 - mse: 0.1246 - auc_19: 0.8919\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3886 - accuracy: 0.8295 - mae: 0.2460 - mse: 0.1223 - auc_19: 0.8957\n",
      "82\n",
      "82\n",
      "81\n",
      "81\n",
      "80\n",
      "80\n",
      "79\n",
      "78\n",
      "75\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3835 - accuracy: 0.8299 - mae: 0.2430 - mse: 0.1209 - auc_19: 0.8985\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3780 - accuracy: 0.8342 - mae: 0.2387 - mse: 0.1188 - auc_19: 0.9016\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3723 - accuracy: 0.8365 - mae: 0.2353 - mse: 0.1171 - auc_19: 0.9048\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3705 - accuracy: 0.8366 - mae: 0.2340 - mse: 0.1167 - auc_19: 0.9062\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3655 - accuracy: 0.8391 - mae: 0.2311 - mse: 0.1152 - auc_19: 0.9086\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3616 - accuracy: 0.8406 - mae: 0.2282 - mse: 0.1139 - auc_19: 0.9108\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3597 - accuracy: 0.8429 - mae: 0.2269 - mse: 0.1130 - auc_19: 0.9120\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3575 - accuracy: 0.8426 - mae: 0.2256 - mse: 0.1123 - auc_19: 0.9133\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3578 - accuracy: 0.8423 - mae: 0.2259 - mse: 0.1126 - auc_19: 0.9130\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3543 - accuracy: 0.8431 - mae: 0.2236 - mse: 0.1116 - auc_19: 0.9148\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "82\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3627 - accuracy: 0.8387 - mae: 0.2302 - mse: 0.1146 - auc_19: 0.9106\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3608 - accuracy: 0.8404 - mae: 0.2284 - mse: 0.1141 - auc_19: 0.9116\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3584 - accuracy: 0.8398 - mae: 0.2273 - mse: 0.1135 - auc_19: 0.9128\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3606 - accuracy: 0.8401 - mae: 0.2287 - mse: 0.1142 - auc_19: 0.9116\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3578 - accuracy: 0.8414 - mae: 0.2270 - mse: 0.1132 - auc_19: 0.9130\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3566 - accuracy: 0.8417 - mae: 0.2264 - mse: 0.1129 - auc_19: 0.9137\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3544 - accuracy: 0.8428 - mae: 0.2248 - mse: 0.1120 - auc_19: 0.9148\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3622 - accuracy: 0.8373 - mae: 0.2307 - mse: 0.1151 - auc_19: 0.9106\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3603 - accuracy: 0.8386 - mae: 0.2293 - mse: 0.1145 - auc_19: 0.9115\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3588 - accuracy: 0.8390 - mae: 0.2279 - mse: 0.1138 - auc_19: 0.9124\n",
      "83\n",
      "83\n",
      "84\n",
      "84\n",
      "84\n",
      "84\n",
      "83\n",
      "84\n",
      "83\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3562 - accuracy: 0.8397 - mae: 0.2268 - mse: 0.1129 - auc_19: 0.9137\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3556 - accuracy: 0.8401 - mae: 0.2265 - mse: 0.1132 - auc_19: 0.9139\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3558 - accuracy: 0.8409 - mae: 0.2262 - mse: 0.1129 - auc_19: 0.9140\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3612 - accuracy: 0.8366 - mae: 0.2306 - mse: 0.1152 - auc_19: 0.9110\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3728 - accuracy: 0.8310 - mae: 0.2385 - mse: 0.1191 - auc_19: 0.9048\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3732 - accuracy: 0.8305 - mae: 0.2394 - mse: 0.1193 - auc_19: 0.9046\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3764 - accuracy: 0.8291 - mae: 0.2413 - mse: 0.1205 - auc_19: 0.9028\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3742 - accuracy: 0.8301 - mae: 0.2402 - mse: 0.1199 - auc_19: 0.9038\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3716 - accuracy: 0.8313 - mae: 0.2380 - mse: 0.1188 - auc_19: 0.9053\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3711 - accuracy: 0.8312 - mae: 0.2380 - mse: 0.1188 - auc_19: 0.9056\n",
      "83\n",
      "83\n",
      "82\n",
      "83\n",
      "83\n",
      "83\n",
      "84\n",
      "84\n",
      "83\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3704 - accuracy: 0.8321 - mae: 0.2375 - mse: 0.1186 - auc_19: 0.9060\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3668 - accuracy: 0.8329 - mae: 0.2350 - mse: 0.1173 - auc_19: 0.9080\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3653 - accuracy: 0.8338 - mae: 0.2344 - mse: 0.1169 - auc_19: 0.9086\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3715 - accuracy: 0.8319 - mae: 0.2380 - mse: 0.1189 - auc_19: 0.9053\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3799 - accuracy: 0.8255 - mae: 0.2446 - mse: 0.1221 - auc_19: 0.9004\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3900 - accuracy: 0.8189 - mae: 0.2526 - mse: 0.1262 - auc_19: 0.8943\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3902 - accuracy: 0.8178 - mae: 0.2530 - mse: 0.1265 - auc_19: 0.8940\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3896 - accuracy: 0.8183 - mae: 0.2526 - mse: 0.1260 - auc_19: 0.8947\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3875 - accuracy: 0.8187 - mae: 0.2513 - mse: 0.1255 - auc_19: 0.8956\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3910 - accuracy: 0.8168 - mae: 0.2538 - mse: 0.1266 - auc_19: 0.8937\n",
      "81\n",
      "81\n",
      "81\n",
      "81\n",
      "82\n",
      "83\n",
      "83\n",
      "83\n",
      "83\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4008 - accuracy: 0.8117 - mae: 0.2604 - mse: 0.1300 - auc_19: 0.8879\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4067 - accuracy: 0.8099 - mae: 0.2645 - mse: 0.1320 - auc_19: 0.8845\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4093 - accuracy: 0.8068 - mae: 0.2665 - mse: 0.1332 - auc_19: 0.8827\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4075 - accuracy: 0.8072 - mae: 0.2656 - mse: 0.1326 - auc_19: 0.8836\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4064 - accuracy: 0.8082 - mae: 0.2649 - mse: 0.1321 - auc_19: 0.8845\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4053 - accuracy: 0.8081 - mae: 0.2639 - mse: 0.1319 - auc_19: 0.8850\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4049 - accuracy: 0.8075 - mae: 0.2640 - mse: 0.1318 - auc_19: 0.8851\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4032 - accuracy: 0.8096 - mae: 0.2628 - mse: 0.1312 - auc_19: 0.8861\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4026 - accuracy: 0.8094 - mae: 0.2620 - mse: 0.1308 - auc_19: 0.8869\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4000 - accuracy: 0.8102 - mae: 0.2604 - mse: 0.1300 - auc_19: 0.8879\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "81\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4012 - accuracy: 0.8095 - mae: 0.2613 - mse: 0.1306 - auc_19: 0.8873\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3994 - accuracy: 0.8108 - mae: 0.2602 - mse: 0.1301 - auc_19: 0.8882\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3980 - accuracy: 0.8113 - mae: 0.2599 - mse: 0.1297 - auc_19: 0.8889\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3979 - accuracy: 0.8113 - mae: 0.2594 - mse: 0.1295 - auc_19: 0.8892\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3980 - accuracy: 0.8114 - mae: 0.2598 - mse: 0.1297 - auc_19: 0.8889\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3971 - accuracy: 0.8107 - mae: 0.2594 - mse: 0.1295 - auc_19: 0.8893\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4007 - accuracy: 0.8089 - mae: 0.2621 - mse: 0.1309 - auc_19: 0.8870\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4087 - accuracy: 0.8038 - mae: 0.2673 - mse: 0.1337 - auc_19: 0.8822\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4063 - accuracy: 0.8053 - mae: 0.2657 - mse: 0.1327 - auc_19: 0.8838\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4065 - accuracy: 0.8062 - mae: 0.2660 - mse: 0.1328 - auc_19: 0.8837\n",
      "80\n",
      "80\n",
      "80\n",
      "81\n",
      "81\n",
      "81\n",
      "81\n",
      "81\n",
      "80\n",
      "Epoch 1/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4049 - accuracy: 0.8047 - mae: 0.2654 - mse: 0.1325 - auc_19: 0.8843\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4046 - accuracy: 0.8054 - mae: 0.2648 - mse: 0.1323 - auc_19: 0.8845\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4034 - accuracy: 0.8065 - mae: 0.2644 - mse: 0.1320 - auc_19: 0.8852\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4041 - accuracy: 0.8048 - mae: 0.2650 - mse: 0.1324 - auc_19: 0.8846\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4027 - accuracy: 0.8072 - mae: 0.2634 - mse: 0.1317 - auc_19: 0.8857\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4016 - accuracy: 0.8075 - mae: 0.2634 - mse: 0.1314 - auc_19: 0.8861\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4011 - accuracy: 0.8082 - mae: 0.2622 - mse: 0.1311 - auc_19: 0.8867\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4007 - accuracy: 0.8081 - mae: 0.2626 - mse: 0.1311 - auc_19: 0.8867\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.3993 - accuracy: 0.8087 - mae: 0.2618 - mse: 0.1307 - auc_19: 0.8873\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 3s 4ms/step - loss: 0.4016 - accuracy: 0.8084 - mae: 0.2631 - mse: 0.1313 - auc_19: 0.8863\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "8\n",
      "244/244 [==============================] - 0s 636us/step - loss: 0.3202 - accuracy: 0.8471 - mae: 0.2136 - mse: 0.1007 - auc_19: 0.8215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Epoch_S = 10\n",
    "\n",
    "def evaluate_model(df, k = 10 , shuffle = False):\n",
    "    result =[]    \n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle= shuffle, random_state=None)\n",
    "    \n",
    "    for train_index, test_index in kf.split(df):\n",
    "\n",
    "        train_ds = [df[index] for index in train_index] \n",
    "        \n",
    "        valid_ds = [df[index] for index in test_index]\n",
    "        \n",
    "        label_pos , label_neg, _, _ = count_lablel(train_ds)\n",
    "        print(f'train positive label: {label_pos} - train negative label: {label_neg}')\n",
    "        \n",
    "        train_ds = up_and_down_Samplenig(train_ds, scale_downsampling = 0.5)\n",
    "        \n",
    "        label_pos , label_neg , _,_ = count_lablel(train_ds)\n",
    "        print(f'up and down sampling => train positive label: {label_pos} - train negative label: {label_neg}')\n",
    "\n",
    "        label_pos , label_neg, _,_ = count_lablel(valid_ds)\n",
    "        print(f'Test positive label: {label_pos} - Test negative label: {label_neg}')\n",
    "\n",
    "        l_train = []\n",
    "        r_train = []\n",
    "        lbls_train = []\n",
    "        l_valid = []\n",
    "        r_valid = []\n",
    "        lbls_valid = []\n",
    "\n",
    "        for i , data in enumerate(train_ds):\n",
    "            embbed_drug, onehot_task, embbed_task, lbl, task_name = data\n",
    "            l_train.append(embbed_drug[0])\n",
    "            r_train.append(embbed_task)\n",
    "            lbls_train.append(lbl.tolist())\n",
    "        \n",
    "        for i , data in enumerate(valid_ds):\n",
    "            embbed_drug, onehot_task, embbed_task, lbl, task_name = data\n",
    "            l_valid.append(embbed_drug[0])\n",
    "            r_valid.append(embbed_task)\n",
    "            lbls_valid.append(lbl.tolist())\n",
    "\n",
    "        l_train = np.array(l_train).reshape(-1,128,1)\n",
    "        r_train = np.array(r_train).reshape(-1,512,1)\n",
    "        lbls_train = np.array(lbls_train)\n",
    "\n",
    "        l_valid = np.array(l_valid).reshape(-1,128,1)\n",
    "        r_valid = np.array(r_valid).reshape(-1,512,1)\n",
    "        lbls_valid = np.array(lbls_valid)\n",
    "\n",
    "        # create neural network model\n",
    "        siamese_net = siamese_model_Canonical_tox21()\n",
    "        history = History()\n",
    "        P = siamese_net.fit([l_train, r_train], lbls_train, epochs = Epoch_S, batch_size = 128, callbacks=[history])\n",
    "\n",
    "        for j in range(100):\n",
    "            C=1\n",
    "            Before = int(P.history['accuracy'][-1]*100)\n",
    "            for i in range(2,Epoch_S+1):\n",
    "                if  int(P.history['accuracy'][-i]*100) == Before:\n",
    "                    C=C+1\n",
    "                else:\n",
    "                    C=1\n",
    "                Before=int(P.history['accuracy'][-i]*100)\n",
    "                print(Before)\n",
    "            if C==Epoch_S:\n",
    "                break\n",
    "            P = siamese_net.fit([l_train, r_train], lbls_train, epochs = Epoch_S, batch_size = 128, callbacks=[history])\n",
    "        print(j+1)\n",
    "        \n",
    "        score  = siamese_net.evaluate([l_valid,r_valid], lbls_valid, verbose=1)\n",
    "        a = (score[1],score[4])\n",
    "        result.append(a)\n",
    "    \n",
    "    return result\n",
    "\n",
    "scores = evaluate_model(data_ds, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "#### Dropout = 0.3 and downsampling = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8456702828407288, 0.8494813442230225),\n",
       " (0.8328415751457214, 0.8326378464698792),\n",
       " (0.8669660091400146, 0.880422055721283),\n",
       " (0.8672226071357727, 0.8763396739959717),\n",
       " (0.8470814824104309, 0.8625595569610596),\n",
       " (0.8715843558311462, 0.8727011680603027),\n",
       " (0.8500128388404846, 0.8268623352050781),\n",
       " (0.8700281977653503, 0.8622535467147827),\n",
       " (0.8677187561988831, 0.8788975477218628),\n",
       " (0.8670772314071655, 0.8792339563369751)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.8586203336715699 AUC= 0.8621389031410217 STD_AUC= 0.018696904807838016\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "auc = []\n",
    "for i in scores:\n",
    "    acc.append(i[0])\n",
    "    auc.append(i[1])\n",
    "\n",
    "print(f'accuracy= {np.mean(acc)} AUC= {np.mean(auc)} STD_AUC= {np.std(auc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dropout = 0.2 and downsampling = 0.5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8568313121795654, 0.8621705770492554),\n",
       " (0.8556767106056213, 0.8610098361968994),\n",
       " (0.8727389574050903, 0.8780911564826965),\n",
       " (0.85516357421875, 0.8480740785598755),\n",
       " (0.8622193932533264, 0.8729240894317627),\n",
       " (0.8745349645614624, 0.8670076131820679),\n",
       " (0.8867077231407166, 0.8892626762390137),\n",
       " (0.8628432154655457, 0.8677256107330322),\n",
       " (0.8418014049530029, 0.8095808029174805),\n",
       " (0.847061812877655, 0.8214605450630188)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.8615579068660736 AUC= 0.8577306985855102 STD_AUC= 0.02362893239897568\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "auc = []\n",
    "for i in scores:\n",
    "    acc.append(i[0])\n",
    "    auc.append(i[1])\n",
    "\n",
    "print(f'accuracy= {np.mean(acc)} AUC= {np.mean(auc)} STD_AUC= {np.std(auc)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zIKQi__XAcia"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
